{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4730922",
   "metadata": {},
   "source": [
    "# Colab: Binance Futures Direction Model (LightGBM, Memory‑Efficient)\n",
    "\n",
    "**Goals**\n",
    "- Be *reliably memory‑safe* on Colab.\n",
    "- Stream & cache heavy datasets (trades/aggTrades/bookDepth) **per file → 15m** before merging.\n",
    "- Load only needed columns and downcast to **float32**.\n",
    "- Train a calibrated **LightGBM** classifier for next‑`n` direction.\n",
    "\n",
    "**Workflow**\n",
    "1) Mount Drive & discover `/content/drive/MyDrive/binance_data`.\n",
    "2) Process **klines** as the base 15m frame.\n",
    "3) For heavy sources: **process file-by-file → resample(15m) → cache parquet** under `/content/drive/MyDrive/binance_cache`.\n",
    "4) Merge small cached frames and train.\n",
    "\n",
    "Toggle heavy sources with flags if RAM is tight, then re‑enable; caches persist across runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5133b2",
   "metadata": {},
   "source": [
    "## 0) Setup & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bacdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install lightgbm==4.3.0 pyarrow==16.1.0 fastparquet==2024.5.0\n",
    "import os, sys, json, gc, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3e0ab",
   "metadata": {},
   "source": [
    "## 1) Configure paths & discover Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('/content/drive/MyDrive/binance_data')\n",
    "CACHE_DIR = Path('/content/drive/MyDrive/binance_cache')\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "assert DATA_ROOT.exists(), f'Data root not found: {DATA_ROOT}'\n",
    "\n",
    "def build_manifest(root: Path) -> dict:\n",
    "    manifest = {k: [] for k in [\n",
    "        'klines','markpriceklines','indexpriceklines','premiumindexklines',\n",
    "        'metrics','trades','aggtrades','bookdepth','other']}\n",
    "    for p in root.rglob('*.parquet'):\n",
    "        name = p.name.lower()\n",
    "        if 'markprice' in name and 'kline' in name:\n",
    "            manifest['markpriceklines'].append(str(p))\n",
    "        elif 'indexprice' in name and 'kline' in name:\n",
    "            manifest['indexpriceklines'].append(str(p))\n",
    "        elif 'premiumindex' in name and 'kline' in name:\n",
    "            manifest['premiumindexklines'].append(str(p))\n",
    "        elif 'kline' in name or 'klines' in name:\n",
    "            manifest['klines'].append(str(p))\n",
    "        elif 'aggtrade' in name:\n",
    "            manifest['aggtrades'].append(str(p))\n",
    "        elif 'bookdepth' in name or 'orderbook' in name:\n",
    "            manifest['bookdepth'].append(str(p))\n",
    "        elif 'metrics' in name or 'open_interest' in name:\n",
    "            manifest['metrics'].append(str(p))\n",
    "        elif 'trade' in name:\n",
    "            manifest['trades'].append(str(p))\n",
    "        else:\n",
    "            manifest['other'].append(str(p))\n",
    "    return manifest\n",
    "\n",
    "manifest = build_manifest(DATA_ROOT)\n",
    "for k,v in manifest.items():\n",
    "    print(f\"{k}: {len(v)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4138cb",
   "metadata": {},
   "source": [
    "## 2) Memory‑safe loaders, robust time handling, and per‑file caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_FREQ = '15T'  # 15‑minute bars\n",
    "\n",
    "def _cache_path(kind: str, src_path: str) -> Path:\n",
    "    fn = Path(src_path).name.replace('.parquet', f'.{kind}.15m.parquet')\n",
    "    return CACHE_DIR / fn\n",
    "\n",
    "def detect_time_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def to_utc_index(df, preferred_time_cols=('open_time','time','timestamp','create_time','transact_time','close_time'), reconstruct_from_close=False):\n",
    "    if df.empty: return df\n",
    "    tcol = detect_time_col(df, preferred_time_cols)\n",
    "    if reconstruct_from_close and (tcol is None or tcol == 'close_time'):\n",
    "        if 'close_time' in df.columns:\n",
    "            close_ts = pd.to_datetime(df['close_time'], utc=True, errors='coerce')\n",
    "            df = df.copy(); df['open_time'] = close_ts - pd.Timedelta(BAR_FREQ); tcol = 'open_time'\n",
    "        else:\n",
    "            raise ValueError('Need close_time to reconstruct open_time.')\n",
    "    if tcol is None:\n",
    "        raise AssertionError(f'Expected one of {preferred_time_cols}, got {list(df.columns)[:10]} ...')\n",
    "    df[tcol] = pd.to_datetime(df[tcol], utc=True, errors='coerce')\n",
    "    df = df.dropna(subset=[tcol]).sort_values(tcol).drop_duplicates(tcol)\n",
    "    df = df.set_index(tcol)\n",
    "    return df.tz_localize('UTC') if df.index.tz is None else df.tz_convert('UTC')\n",
    "\n",
    "def load_concat(files, columns=None):\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f, columns=columns) if columns else pd.read_parquet(f)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print('Failed to load', f, e)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def _safe_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def _to_bool_series(s):\n",
    "    if s.dtype == 'bool': return s\n",
    "    mapping_true  = {True, 'true', 'True', 1, '1'}\n",
    "    mapping_false = {False, 'false', 'False', 0, '0'}\n",
    "    return s.map(lambda x: True if x in mapping_true else (False if x in mapping_false else False)).fillna(False).astype(bool)\n",
    "\n",
    "def aggregate_trades_files(files):\n",
    "    outs = []\n",
    "    for f in files:\n",
    "        cpath = _cache_path('trades', f)\n",
    "        if cpath.exists():\n",
    "            outs.append(pd.read_parquet(cpath)); continue\n",
    "        try:\n",
    "            df = pd.read_parquet(f, columns=['price','qty','quote_qty','time','is_buyer_maker'])\n",
    "        except:\n",
    "            df = pd.read_parquet(f)\n",
    "        df = to_utc_index(df, preferred_time_cols=('time','timestamp'))\n",
    "        if df.empty: continue\n",
    "        df = _safe_numeric(df, ['price','qty','quote_qty'])\n",
    "        if 'is_buyer_maker' in df.columns: df['is_buyer_maker'] = _to_bool_series(df['is_buyer_maker'])\n",
    "        df['dollar'] = (df.get('price',0) * df.get('qty',0)).fillna(0)\n",
    "        g = df.resample(BAR_FREQ)\n",
    "        out = pd.DataFrame({\n",
    "            'trades_count': g.size(),\n",
    "            'qty_sum':      g['qty'].sum(min_count=1),\n",
    "            'dollar_sum':   g['dollar'].sum(min_count=1),\n",
    "            'vwap':         g.apply(lambda x: (x['price']*x['qty']).sum()/max(x['qty'].sum(),1e-9) if 'price' in x and 'qty' in x else np.nan)\n",
    "        })\n",
    "        if 'is_buyer_maker' in df.columns:\n",
    "            def side_sum(x, side_bool):\n",
    "                if len(x)==0: return 0.0\n",
    "                return x.loc[x['is_buyer_maker']==side_bool, 'qty'].sum()\n",
    "            out['buy_qty']  = g.apply(lambda x: side_sum(x, False))\n",
    "            out['sell_qty'] = g.apply(lambda x: side_sum(x, True))\n",
    "            out['ofi']      = (out['buy_qty'] - out['sell_qty']) / (out['qty_sum'] + 1e-9)\n",
    "        else:\n",
    "            out['buy_qty'] = 0.0; out['sell_qty'] = 0.0; out['ofi'] = 0.0\n",
    "        out = out.astype('float32')\n",
    "        out.to_parquet(cpath)\n",
    "        outs.append(out)\n",
    "        del df, out; gc.collect()\n",
    "    if not outs: return pd.DataFrame()\n",
    "    df_all = pd.concat(outs).sort_index()\n",
    "    return df_all.groupby(df_all.index).sum()\n",
    "\n",
    "def aggregate_aggtrades_files(files):\n",
    "    outs = []\n",
    "    for f in files:\n",
    "        cpath = _cache_path('aggtrades', f)\n",
    "        if cpath.exists():\n",
    "            outs.append(pd.read_parquet(cpath)); continue\n",
    "        try:\n",
    "            df = pd.read_parquet(f, columns=['price','quantity','transact_time'])\n",
    "        except:\n",
    "            df = pd.read_parquet(f)\n",
    "        df = to_utc_index(df, preferred_time_cols=('transact_time','time','timestamp'))\n",
    "        if df.empty: continue\n",
    "        df = df.rename(columns={'quantity':'qty'})\n",
    "        df = _safe_numeric(df, ['price','qty'])\n",
    "        df['dollar'] = (df.get('price',0) * df.get('qty',0)).fillna(0)\n",
    "        g = df.resample(BAR_FREQ)\n",
    "        out = pd.DataFrame({\n",
    "            'agg_count':      g.size(),\n",
    "            'agg_qty_sum':    g['qty'].sum(min_count=1),\n",
    "            'agg_dollar_sum': g['dollar'].sum(min_count=1),\n",
    "            'agg_vwap':       g.apply(lambda x: (x['price']*x['qty']).sum()/max(x['qty'].sum(),1e-9) if 'price' in x and 'qty' in x else np.nan)\n",
    "        }).astype('float32')\n",
    "        out.to_parquet(cpath)\n",
    "        outs.append(out)\n",
    "        del df, out; gc.collect()\n",
    "    if not outs: return pd.DataFrame()\n",
    "    df_all = pd.concat(outs).sort_index()\n",
    "    return df_all.groupby(df_all.index).sum()\n",
    "\n",
    "def aggregate_bookdepth_files(files):\n",
    "    outs = []\n",
    "    for f in files:\n",
    "        cpath = _cache_path('bookdepth', f)\n",
    "        if cpath.exists():\n",
    "            outs.append(pd.read_parquet(cpath)); continue\n",
    "        try:\n",
    "            df = pd.read_parquet(f, columns=['timestamp','percentage','depth','notional'])\n",
    "        except:\n",
    "            df = pd.read_parquet(f)\n",
    "        df = to_utc_index(df, preferred_time_cols=('timestamp','time'))\n",
    "        if df.empty: continue\n",
    "        df = _safe_numeric(df, ['percentage','depth','notional'])\n",
    "        g = df.resample(BAR_FREQ)\n",
    "        out = pd.DataFrame({\n",
    "            'bd_notional_sum': g['notional'].sum(min_count=1),\n",
    "            'bd_depth_sum':    g['depth'].sum(min_count=1)\n",
    "        }).astype('float32')\n",
    "        out.to_parquet(cpath)\n",
    "        outs.append(out)\n",
    "        del df, out; gc.collect()\n",
    "    if not outs: return pd.DataFrame()\n",
    "    df_all = pd.concat(outs).sort_index()\n",
    "    return df_all.groupby(df_all.index).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ea341",
   "metadata": {},
   "source": [
    "## 3) Load datasets with memory toggles (safe defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle heavy sources off initially; enable one-by-one after caches are built\n",
    "USE_KLINES = True\n",
    "USE_MARK_INDEX_PREMIUM = True\n",
    "USE_METRICS = True\n",
    "USE_TRADES = False      # set True after first light run\n",
    "USE_AGGTRADES = False   # set True after first light run\n",
    "USE_BOOKDEPTH = False   # set True after first light run\n",
    "\n",
    "# Base klines\n",
    "if USE_KLINES:\n",
    "    kl_raw = load_concat(manifest['klines'], columns=['open_time','close_time','open','high','low','close','volume'])\n",
    "    print('klines raw:', kl_raw.shape)\n",
    "    kl_df = to_utc_index(kl_raw, preferred_time_cols=('open_time','time','timestamp','close_time'), reconstruct_from_close=True)\n",
    "    keep_cols = [c for c in ['open','high','low','close','volume'] if c in kl_df.columns]\n",
    "    assert len(keep_cols) >= 4, 'Expected at least open/high/low/close in klines.'\n",
    "    kl_df = kl_df[keep_cols].astype('float32')\n",
    "    print('klines aligned:', kl_df.shape)\n",
    "else:\n",
    "    raise RuntimeError('Klines are required as the base.')\n",
    "\n",
    "# Mark/Index/Premium (bar → small)\n",
    "if USE_MARK_INDEX_PREMIUM:\n",
    "    mk_df = to_utc_index(load_concat(manifest['markpriceklines'], columns=['open_time','close_time','open','high','low','close','volume']))\n",
    "    ix_df = to_utc_index(load_concat(manifest['indexpriceklines'], columns=['open_time','close_time','open','high','low','close','volume']))\n",
    "    pr_df = to_utc_index(load_concat(manifest['premiumindexklines'], columns=['open_time','close_time','open','high','low','close','volume']))\n",
    "    def sel_bars(df):\n",
    "        return df[[c for c in ['open','high','low','close','volume'] if c in df.columns]].astype('float32') if not df.empty else df\n",
    "    mark_k, index_k, prem_k = sel_bars(mk_df), sel_bars(ix_df), sel_bars(pr_df)\n",
    "else:\n",
    "    mark_k = index_k = prem_k = pd.DataFrame()\n",
    "\n",
    "# Metrics (sparse → ffill 15m)\n",
    "if USE_METRICS:\n",
    "    mt_raw = load_concat(manifest['metrics'], columns=['create_time','sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio'])\n",
    "    mt_df  = to_utc_index(mt_raw, preferred_time_cols=('create_time','time','timestamp'))\n",
    "    if not mt_df.empty:\n",
    "        metrics_cols = [c for c in ['sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio'] if c in mt_df.columns]\n",
    "        mt_15 = mt_df[metrics_cols].astype('float32').resample(BAR_FREQ).ffill()\n",
    "    else:\n",
    "        mt_15 = pd.DataFrame()\n",
    "else:\n",
    "    mt_15 = pd.DataFrame()\n",
    "\n",
    "# Heavy sources: stream → cache → merge\n",
    "tr_15 = aggregate_trades_files(manifest['trades'])   if USE_TRADES    and manifest['trades']    else pd.DataFrame()\n",
    "ag_15 = aggregate_aggtrades_files(manifest['aggtrades']) if USE_AGGTRADES and manifest['aggtrades'] else pd.DataFrame()\n",
    "bd_15 = aggregate_bookdepth_files(manifest['bookdepth']) if USE_BOOKDEPTH and manifest['bookdepth'] else pd.DataFrame()\n",
    "\n",
    "print('Shapes — kl:', kl_df.shape,\n",
    "      'mark:', getattr(mark_k,'shape',()), 'index:', getattr(index_k,'shape',()), 'prem:', getattr(prem_k,'shape',()))\n",
    "print('Shapes — metrics15:', mt_15.shape, 'trades15:', tr_15.shape, 'aggtrades15:', ag_15.shape, 'bookdepth15:', bd_15.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955f555",
   "metadata": {},
   "source": [
    "## 4) Merge into unified 15m base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb41566",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = kl_df.copy()\n",
    "\n",
    "def safe_join(left, right):\n",
    "    return left.join(right, how='left') if not right.empty else left\n",
    "\n",
    "if not mark_k.empty:  base = safe_join(base, mark_k[['close']].rename(columns={'close':'mark_close'}))\n",
    "if not index_k.empty: base = safe_join(base, index_k[['close']].rename(columns={'close':'index_close'}))\n",
    "if not prem_k.empty:  base = safe_join(base, prem_k[['close']].rename(columns={'close':'premium_close'}))\n",
    "base = safe_join(base, mt_15)\n",
    "base = safe_join(base, tr_15)\n",
    "base = safe_join(base, ag_15)\n",
    "base = safe_join(base, bd_15)\n",
    "\n",
    "base = base.sort_index().replace([np.inf,-np.inf], np.nan).fillna(method='ffill').fillna(0)\n",
    "print('Unified base shape:', base.shape)\n",
    "base.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb12ef8",
   "metadata": {},
   "source": [
    "## 5) Triple‑barrier labels (next n bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HORIZON = 16   # next n=16 bars (~4h on 15m)\n",
    "K_UP = 1.5; K_DN = 1.5; ATR_LEN = 14\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, atr_len=14):\n",
    "    high, low, close = df['high'], df['low'], df['close']\n",
    "    tr = pd.concat([(high-low).abs(), (high-close.shift()).abs(), (low-close.shift()).abs()], axis=1).max(axis=1)\n",
    "    return tr.ewm(span=atr_len, adjust=False).mean()\n",
    "\n",
    "def triple_barrier_labels(df, n=16, k_up=1.5, k_dn=1.5, atr_len=14):\n",
    "    atr = compute_atr(df, atr_len=atr_len)\n",
    "    price = df['close'].values\n",
    "    up_m = (k_up * atr / df['close']).fillna(method='bfill').values\n",
    "    dn_m = (k_dn * atr / df['close']).fillna(method='bfill').values\n",
    "    y = np.full(len(df), 2, dtype=np.int8)\n",
    "    hi, lo = df['high'].values, df['low'].values\n",
    "    L = len(df)\n",
    "    for i in range(max(0, L-n)):\n",
    "        p0 = price[i]\n",
    "        up = p0 * (1 + up_m[i]); dn = p0 * (1 - dn_m[i])\n",
    "        hp = hi[i+1:i+n+1]; lp = lo[i+1:i+n+1]\n",
    "        iu = np.where(hp >= up)[0]; idn = np.where(lp <= dn)[0]\n",
    "        if iu.size and (not idn.size or iu[0] < idn[0]): y[i] = 1\n",
    "        elif idn.size and (not iu.size or idn[0] < iu[0]): y[i] = 0\n",
    "        else: y[i] = 2\n",
    "    labels = pd.Series(y, index=df.index, name='label'); labels.iloc[-n:] = 2\n",
    "    return labels\n",
    "\n",
    "labels = triple_barrier_labels(base, n=N_HORIZON, k_up=K_UP, k_dn=K_DN, atr_len=ATR_LEN)\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd191a1c",
   "metadata": {},
   "source": [
    "## 6) Lightweight features (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame):\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X['ret_1'] = df['close'].pct_change()\n",
    "    for lag in [2,4,8,16,32,64]:\n",
    "        X[f'ret_{lag}'] = df['close'].pct_change(lag)\n",
    "    atr14 = compute_atr(df, atr_len=14)\n",
    "    X['atr14p'] = (atr14/df['close'])\n",
    "    ema8 = df['close'].ewm(span=8).mean(); ema21 = df['close'].ewm(span=21).mean()\n",
    "    X['ema_diff'] = ema8 - ema21\n",
    "    delta = df['close'].diff(); up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    rs = up.ewm(span=14).mean() / (down.ewm(span=14).mean() + 1e-9)\n",
    "    X['rsi14'] = 100 - 100/(1+rs)\n",
    "    if 'mark_close' in df.columns:  X['mark_spread_p']  = (df['mark_close']  - df['close'])/df['close']\n",
    "    if 'index_close' in df.columns: X['index_spread_p'] = (df['index_close'] - df['close'])/df['close']\n",
    "    if 'premium_close' in df.columns: X['premium_chg'] = df['premium_close'].pct_change()\n",
    "    if 'sum_open_interest' in df.columns: X['oi_chg_p'] = df['sum_open_interest'].pct_change().fillna(0)\n",
    "    if 'sum_taker_long_short_vol_ratio' in df.columns: X['taker_ls_chg_p'] = df['sum_taker_long_short_vol_ratio'].pct_change().fillna(0)\n",
    "    if 'ofi' in df.columns: X['ofi'] = df['ofi']\n",
    "    if 'trades_count' in df.columns: X['trades_count'] = df['trades_count']\n",
    "    if 'vwap' in df.columns: X['vwap_close_spread'] = (df['vwap'] - df['close'])/df['close']\n",
    "    if 'bd_notional_sum' in df.columns: X['bd_notional_sum'] = df['bd_notional_sum']\n",
    "    X = X.replace([np.inf,-np.inf], np.nan).fillna(method='bfill').fillna(0)\n",
    "    return X.astype('float32')\n",
    "\n",
    "X = engineer_features(base)\n",
    "print('X shape:', X.shape, 'dtype:', X.dtypes.iloc[0])\n",
    "gc.collect(); X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62f67b",
   "metadata": {},
   "source": [
    "## 7) Train LightGBM (early stopping) + isotonic calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91538414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Time-based holdout for calibration\n",
    "n = len(X); cal_size = max(int(n * 0.15), 500); split_idx = n - cal_size\n",
    "X_tr, y_tr = X.iloc[:split_idx], labels.iloc[:split_idx]\n",
    "X_cal, y_cal = X.iloc[split_idx:], labels.iloc[split_idx:]\n",
    "\n",
    "le = LabelEncoder(); y_tr_enc = le.fit_transform(y_tr.values); y_cal_enc = le.transform(y_cal.values)\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    objective='multiclass', num_class=3,\n",
    "    learning_rate=0.03, n_estimators=4000,  # allow many trees w/ early stopping\n",
    "    num_leaves=64, subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "clf.fit(\n",
    "    X_tr, y_tr_enc,\n",
    "    eval_set=[(X_cal, y_cal_enc)],\n",
    "    eval_metric='multi_logloss',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    ")\n",
    "\n",
    "# Isotonic calibration on tail holdout\n",
    "cal = CalibratedClassifierCV(clf, method='isotonic', cv='prefit')\n",
    "cal.fit(X_cal, y_cal_enc)\n",
    "\n",
    "probs_cal = cal.predict_proba(X_cal)\n",
    "up_idx = list(le.classes_).index(1) if 1 in le.classes_ else 0\n",
    "print('Calibration Brier (UP on holdout):', brier_score_loss((y_cal_enc==1).astype(int), probs_cal[:, up_idx]))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31daefbf",
   "metadata": {},
   "source": [
    "## 8) Save artifacts & predict() helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(ARTIFACT_DIR/'lgbm_calibrated.pkl', 'wb') as f: pickle.dump(cal, f)\n",
    "with open(ARTIFACT_DIR/'label_encoder.pkl', 'wb') as f: pickle.dump(le, f)\n",
    "with open(ARTIFACT_DIR/'feature_columns.json', 'w') as f: f.write(json.dumps(list(X.columns)))\n",
    "print('Saved artifacts to', ARTIFACT_DIR)\n",
    "\n",
    "def predict_live(latest_row: pd.Series, calibrated_model, label_encoder, feature_columns):\n",
    "    Xr = latest_row[feature_columns].values.reshape(1,-1)\n",
    "    proba = calibrated_model.predict_proba(Xr)[0]\n",
    "    classes = list(label_encoder.classes_)\n",
    "    out = {}\n",
    "    for cname, cid in [('P_down',0), ('P_up',1), ('P_neutral',2)]:\n",
    "        out[cname] = float(proba[classes.index(cid)]) if cid in classes else np.nan\n",
    "    return out\n",
    "\n",
    "print('Latest probabilities (current row):', predict_live(X.iloc[-1], cal, le, X.columns))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
