{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4459317",
   "metadata": {},
   "source": [
    "# Colab: Binance Futures Turning-Point Pipeline\n",
    "\n",
    "**What this notebook does**\n",
    "- Mounts **Google Drive** and points to `MyDrive/binance_data` (adjustable)\n",
    "- Recursively **lists subfolders/files** (tree view) and builds a **manifest** of Parquet files\n",
    "- Loads/merges **klines** (and optionally other endpoints) into a 15m dataframe\n",
    "- Creates **triple-barrier labels** (volatility-adaptive)\n",
    "- Engineers basic features\n",
    "- Trains a **LightGBM** baseline with **probability calibration**\n",
    "- Provides a **predict()** stub for live usage\n",
    "\n",
    "> Run this in **Google Colab**. If your Drive layout is different, edit `DATA_ROOT`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a19d60",
   "metadata": {},
   "source": [
    "## 0) Setup & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install lightgbm==4.3.0 pyarrow==16.1.0 fastparquet==2024.5.0\n",
    "import os, sys, json, math, gc, pickle, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553dcc8",
   "metadata": {},
   "source": [
    "## 1) Configure path & list folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path('/content/drive/MyDrive/binance_data')  # adjust if needed\n",
    "assert DATA_ROOT.exists(), f'Data root not found: {DATA_ROOT}'\n",
    "\n",
    "def list_dir_structure(root: Path, only_dirs=False, max_depth=10, _depth=0):\n",
    "    if _depth > max_depth: return\n",
    "    try:\n",
    "        entries = sorted(root.iterdir(), key=lambda p: (not p.is_dir(), p.name.lower()))\n",
    "    except Exception as e:\n",
    "        print('Error listing', root, e)\n",
    "        return\n",
    "    for p in entries:\n",
    "        icon = 'ðŸ“‚' if p.is_dir() else 'ðŸ“„'\n",
    "        if only_dirs and not p.is_dir():\n",
    "            continue\n",
    "        print(' ' * (_depth*2) + f\"{icon} {p.name}\")\n",
    "        if p.is_dir():\n",
    "            list_dir_structure(p, only_dirs=only_dirs, max_depth=max_depth, _depth=_depth+1)\n",
    "\n",
    "print('Root:', DATA_ROOT)\n",
    "print('\\n--- DIRECTORY TREE (folders only) ---')\n",
    "list_dir_structure(DATA_ROOT, only_dirs=True)\n",
    "print('\\n--- DIRECTORY TREE (folders & files) ---')\n",
    "list_dir_structure(DATA_ROOT, only_dirs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b0dd9",
   "metadata": {},
   "source": [
    "## 2) Build manifest of Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "def build_manifest(root: Path) -> dict:\n",
    "    manifest = {\n",
    "        'klines': [], 'markpriceklines': [], 'indexpriceklines': [], 'premiumindexklines': [],\n",
    "        'trades': [], 'aggtrades': [], 'bookdepth': [], 'metrics': [], 'other': []\n",
    "    }\n",
    "    for p in root.rglob('*.parquet'):\n",
    "        name = p.name.lower()\n",
    "        if 'markprice' in name and 'kline' in name:\n",
    "            manifest['markpriceklines'].append(str(p))\n",
    "        elif 'indexprice' in name and 'kline' in name:\n",
    "            manifest['indexpriceklines'].append(str(p))\n",
    "        elif 'premiumindex' in name and 'kline' in name:\n",
    "            manifest['premiumindexklines'].append(str(p))\n",
    "        elif 'kline' in name or 'klines' in name:\n",
    "            manifest['klines'].append(str(p))\n",
    "        elif 'aggtrade' in name:\n",
    "            manifest['aggtrades'].append(str(p))\n",
    "        elif 'bookdepth' in name or 'orderbook' in name:\n",
    "            manifest['bookdepth'].append(str(p))\n",
    "        elif 'metrics' in name or 'open_interest' in name:\n",
    "            manifest['metrics'].append(str(p))\n",
    "        elif 'trade' in name:\n",
    "            manifest['trades'].append(str(p))\n",
    "        else:\n",
    "            manifest['other'].append(str(p))\n",
    "    return manifest\n",
    "\n",
    "manifest = build_manifest(DATA_ROOT)\n",
    "for k,v in manifest.items():\n",
    "    print(f\"{k}: {len(v)} files\")\n",
    "manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f36f06",
   "metadata": {},
   "source": [
    "## 3) Load & concatenate klines (15m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cccea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "def load_concat_parquet(files):\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print('Failed to load', f, e)\n",
    "    if not dfs:\n",
    "        raise RuntimeError('No parquet files loaded')\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "kl_files = manifest['klines']\n",
    "assert len(kl_files) > 0, 'No kline parquet files found. Ensure filenames include kline/klines.'\n",
    "kl_df = load_concat_parquet(kl_files)\n",
    "print('klines shape:', kl_df.shape)\n",
    "print('Columns:', kl_df.columns.tolist())\n",
    "\n",
    "time_col = 'open_time' if 'open_time' in kl_df.columns else ('time' if 'time' in kl_df.columns else None)\n",
    "assert time_col is not None, 'Expected open_time or time column in klines parquet.'\n",
    "kl_df[time_col] = pd.to_datetime(kl_df[time_col], utc=True)\n",
    "kl_df = kl_df.sort_values(time_col).drop_duplicates(time_col)\n",
    "kl_df = kl_df.set_index(time_col)\n",
    "kl_df = kl_df.tz_convert('UTC') if kl_df.index.tz is not None else kl_df.tz_localize('UTC')\n",
    "keep_cols = [c for c in ['open','high','low','close','volume'] if c in kl_df.columns]\n",
    "kl_df = kl_df[keep_cols].astype('float32')\n",
    "kl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4fb97",
   "metadata": {},
   "source": [
    "## 4) Triple-barrier labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atr(df: pd.DataFrame, atr_len=14):\n",
    "    high = df['high']; low = df['low']; close = df['close']\n",
    "    tr1 = (high - low).abs()\n",
    "    tr2 = (high - close.shift()).abs()\n",
    "    tr3 = (low - close.shift()).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(span=atr_len, adjust=False).mean()\n",
    "    return atr\n",
    "\n",
    "def triple_barrier_labels(df, n=16, k_up=1.5, k_dn=1.5, atr_len=14):\n",
    "    atr = compute_atr(df, atr_len=atr_len)\n",
    "    price = df['close'].values\n",
    "    up_mult = (k_up * atr / df['close']).fillna(method='bfill').values\n",
    "    dn_mult = (k_dn * atr / df['close']).fillna(method='bfill').values\n",
    "    y = np.full(len(df), 2, dtype=np.int8)\n",
    "    highs = df['high'].values\n",
    "    lows = df['low'].values\n",
    "    L = len(df)\n",
    "    for i in range(L - n):\n",
    "        p0 = price[i]\n",
    "        up = p0 * (1 + up_mult[i])\n",
    "        dn = p0 * (1 - dn_mult[i])\n",
    "        hi_path = highs[i+1:i+n+1]\n",
    "        lo_path = lows[i+1:i+n+1]\n",
    "        hit_up = np.where(hi_path >= up)[0]\n",
    "        hit_dn = np.where(lo_path <= dn)[0]\n",
    "        if hit_up.size and (not hit_dn.size or hit_up[0] < hit_dn[0]):\n",
    "            y[i] = 1\n",
    "        elif hit_dn.size and (not hit_up.size or hit_dn[0] < hit_up[0]):\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 2\n",
    "    labels = pd.Series(y, index=df.index, name='label')\n",
    "    labels.iloc[-n:] = 2\n",
    "    return labels\n",
    "\n",
    "labels = triple_barrier_labels(kl_df, n=16, k_up=1.5, k_dn=1.5, atr_len=14)\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97507a0",
   "metadata": {},
   "source": [
    "## 5) Feature engineering (examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76aba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X['ret_1'] = df['close'].pct_change()\n",
    "    for lag in [2,4,8,16,32,64]:\n",
    "        X[f'ret_{lag}'] = df['close'].pct_change(lag)\n",
    "    atr14 = compute_atr(df, atr_len=14)\n",
    "    X['atr14p'] = (atr14 / df['close']).fillna(method='bfill')\n",
    "    X['rv_24'] = np.log(df['close']).diff().rolling(96).std()\n",
    "    ema8 = df['close'].ewm(span=8).mean()\n",
    "    ema21 = df['close'].ewm(span=21).mean()\n",
    "    X['ema_diff'] = ema8 - ema21\n",
    "    delta = df['close'].diff()\n",
    "    up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    rs = up.ewm(span=14).mean() / (down.ewm(span=14).mean() + 1e-9)\n",
    "    X['rsi14'] = 100 - 100/(1+rs)\n",
    "    X['vol'] = df['volume']\n",
    "    med24 = X['vol'].rolling(96).median().replace(0, np.nan)\n",
    "    X['vol_surge'] = X['vol'] / (med24 + 1e-9)\n",
    "    return X.fillna(method='bfill').fillna(0)\n",
    "\n",
    "X = make_features(kl_df)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2230dfe",
   "metadata": {},
   "source": [
    "## 6) Train/test split & LightGBM training + calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d03a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_start = pd.Timestamp('2025-02-06', tz='UTC')\n",
    "train_end   = pd.Timestamp('2025-06-06', tz='UTC')\n",
    "test_start  = pd.Timestamp('2025-06-07', tz='UTC')\n",
    "test_end    = pd.Timestamp('2025-08-25', tz='UTC')\n",
    "\n",
    "y = labels.loc[X.index]\n",
    "mask_tr = (X.index >= train_start) & (X.index <= train_end)\n",
    "mask_te = (X.index >= test_start) & (X.index <= test_end)\n",
    "X_train, y_train = X.loc[mask_tr], y.loc[mask_tr]\n",
    "X_test,  y_test  = X.loc[mask_te], y.loc[mask_te]\n",
    "print('Train rows:', len(X_train), 'Test rows:', len(X_test))\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train.values)\n",
    "y_test_enc  = le.transform(y_test.values)\n",
    "\n",
    "params = dict(objective='multiclass', num_class=3, learning_rate=0.03, n_estimators=800,\n",
    "              num_leaves=64, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "clf = lgb.LGBMClassifier(**params)\n",
    "clf.fit(X_train, y_train_enc, eval_set=[(X_test, y_test_enc)], eval_metric='multi_logloss', verbose=False)\n",
    "cal = CalibratedClassifierCV(clf, method='isotonic', cv=3)\n",
    "cal.fit(X_train, y_train_enc)\n",
    "\n",
    "probs = cal.predict_proba(X_test)\n",
    "up_idx = list(le.classes_).index(1) if 1 in le.classes_ else 0\n",
    "print('Test Brier (UP):', brier_score_loss((y_test_enc==1).astype(int), probs[:, up_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26024307",
   "metadata": {},
   "source": [
    "## 7) Predict function & save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_live(latest_row: pd.Series, cal_model, feature_columns):\n",
    "    Xr = latest_row[feature_columns].values.reshape(1,-1)\n",
    "    proba = cal_model.predict_proba(Xr)[0]\n",
    "    return {\n",
    "        'P_down': float(proba[list(le.classes_).index(0)] if 0 in le.classes_ else np.nan),\n",
    "        'P_up': float(proba[list(le.classes_).index(1)] if 1 in le.classes_ else np.nan),\n",
    "        'P_neutral': float(proba[list(le.classes_).index(2)] if 2 in le.classes_ else np.nan)\n",
    "    }\n",
    "\n",
    "if len(X_test) > 0:\n",
    "    res = predict_live(X_test.iloc[-1], cal, X.columns)\n",
    "    print('Latest probs:', res)\n",
    "\n",
    "from pathlib import Path\n",
    "ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(ARTIFACT_DIR/'calibrated_lgbm.pkl', 'wb') as f:\n",
    "    pickle.dump(cal, f)\n",
    "with open(ARTIFACT_DIR/'feature_columns.json', 'w') as f:\n",
    "    f.write(json.dumps(list(X.columns)))\n",
    "print('Saved to', ARTIFACT_DIR)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
