{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9832825",
   "metadata": {},
   "source": [
    "# Colab: Binance Futures Direction Model — **Streaming & Chunked** (LightGBM)\n",
    "\n",
    "**Purpose:** Handle **very large** Binance parquet files **without OOM**.\n",
    "\n",
    "### Key tactics\n",
    "- Use **pyarrow** `ParquetFile.iter_batches()` to read **in chunks**.\n",
    "- For tick/snapshot datasets (`trades`, `aggTrades`, `bookDepth`), we:\n",
    "  - Stream batches → parse time → compute a 15‑min bin (`.floor('15min')`) per row.\n",
    "  - Aggregate **within the chunk** by bin.\n",
    "  - **Accumulate** into a per‑file 15m accumulator (index ≈ tens of thousands rows tops).\n",
    "  - Save a tiny **per‑file cache parquet** to Drive.\n",
    "- Only after caching all heavy files do we merge the small 15m frames and train.\n",
    "\n",
    "Robust timestamp detection + `open_time` reconstruction for klines are preserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff37d3b",
   "metadata": {},
   "source": [
    "## 0) Setup & Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install lightgbm==4.3.0 pyarrow==16.1.0 fastparquet==2024.5.0\n",
    "import os, sys, json, gc, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c13b",
   "metadata": {},
   "source": [
    "## 1) Paths & Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('/content/drive/MyDrive/binance_data')\n",
    "CACHE_DIR = Path('/content/drive/MyDrive/binance_cache')\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "assert DATA_ROOT.exists(), f'Data root not found: {DATA_ROOT}'\n",
    "\n",
    "def build_manifest(root: Path) -> dict:\n",
    "    manifest = {k: [] for k in [\n",
    "        'klines','markpriceklines','indexpriceklines','premiumindexklines',\n",
    "        'metrics','trades','aggtrades','bookdepth','other']}\n",
    "    for p in root.rglob('*.parquet'):\n",
    "        name = p.name.lower()\n",
    "        if 'markprice' in name and 'kline' in name:\n",
    "            manifest['markpriceklines'].append(str(p))\n",
    "        elif 'indexprice' in name and 'kline' in name:\n",
    "            manifest['indexpriceklines'].append(str(p))\n",
    "        elif 'premiumindex' in name and 'kline' in name:\n",
    "            manifest['premiumindexklines'].append(str(p))\n",
    "        elif 'kline' in name or 'klines' in name:\n",
    "            manifest['klines'].append(str(p))\n",
    "        elif 'aggtrade' in name:\n",
    "            manifest['aggtrades'].append(str(p))\n",
    "        elif 'bookdepth' in name or 'orderbook' in name:\n",
    "            manifest['bookdepth'].append(str(p))\n",
    "        elif 'metrics' in name or 'open_interest' in name:\n",
    "            manifest['metrics'].append(str(p))\n",
    "        elif 'trade' in name:\n",
    "            manifest['trades'].append(str(p))\n",
    "        else:\n",
    "            manifest['other'].append(str(p))\n",
    "    return manifest\n",
    "\n",
    "manifest = build_manifest(DATA_ROOT)\n",
    "for k,v in manifest.items():\n",
    "    print(f\"{k}: {len(v)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b00d2",
   "metadata": {},
   "source": [
    "## 2) Streaming helpers & robust time handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ca163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "BAR_FREQ = '15min'\n",
    "\n",
    "def _cache_path(kind: str, src_path: str) -> Path:\n",
    "    fn = Path(src_path).name.replace('.parquet', f'.{kind}.15m.parquet')\n",
    "    return CACHE_DIR / fn\n",
    "\n",
    "def detect_time_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def to_utc_index(df, preferred_time_cols=('open_time','time','timestamp','create_time','transact_time','close_time'), reconstruct_from_close=False):\n",
    "    if df.empty: return df\n",
    "    tcol = detect_time_col(df, preferred_time_cols)\n",
    "    if reconstruct_from_close and (tcol is None or tcol == 'close_time'):\n",
    "        if 'close_time' in df.columns:\n",
    "            df = df.copy(); close_ts = pd.to_datetime(df['close_time'], utc=True, errors='coerce')\n",
    "            df['open_time'] = close_ts - pd.Timedelta(BAR_FREQ); tcol = 'open_time'\n",
    "        else:\n",
    "            raise ValueError('Need close_time to reconstruct open_time.')\n",
    "    if tcol is None:\n",
    "        raise AssertionError(f'Expected one of {preferred_time_cols}, got {list(df.columns)[:10]} ...')\n",
    "    df[tcol] = pd.to_datetime(df[tcol], utc=True, errors='coerce')\n",
    "    df = df.dropna(subset=[tcol]).sort_values(tcol).drop_duplicates(tcol)\n",
    "    return df.set_index(tcol).tz_localize('UTC') if df.index.tz is None else df.set_index(tcol).tz_convert('UTC')\n",
    "\n",
    "def stream_parquet_batches(path, columns=None, batch_size=250_000):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    for batch in pf.iter_batches(batch_size=batch_size, columns=columns):\n",
    "        # Convert to pandas with minimal dtype inflation\n",
    "        yield batch.to_pandas()\n",
    "\n",
    "def _ensure_float32(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce').astype('float32')\n",
    "    return df\n",
    "\n",
    "def _to_bool_series(s):\n",
    "    if s.dtype == 'bool': return s\n",
    "    mapping_true  = {True, 'true', 'True', 1, '1'}\n",
    "    mapping_false = {False, 'false', 'False', 0, '0'}\n",
    "    return s.map(lambda x: True if x in mapping_true else (False if x in mapping_false else False)).fillna(False).astype(bool)\n",
    "\n",
    "# ---------- Chunked aggregators per file ----------\n",
    "def aggregate_trades_file_streaming(path):\n",
    "    acc = None\n",
    "    for chunk in stream_parquet_batches(path, columns=['price','qty','quote_qty','time','is_buyer_maker']):\n",
    "        if chunk.empty: continue\n",
    "        tcol = detect_time_col(chunk, ('time','timestamp')) or 'time'\n",
    "        chunk[tcol] = pd.to_datetime(chunk[tcol], utc=True, errors='coerce')\n",
    "        chunk = chunk.dropna(subset=[tcol]).set_index(tcol)\n",
    "        chunk = _ensure_float32(chunk, ['price','qty','quote_qty'])\n",
    "        if 'is_buyer_maker' in chunk.columns:\n",
    "            chunk['is_buyer_maker'] = _to_bool_series(chunk['is_buyer_maker'])\n",
    "        else:\n",
    "            chunk['is_buyer_maker'] = False\n",
    "        chunk['bin'] = chunk.index.floor(BAR_FREQ)\n",
    "        chunk['px_qty'] = (chunk['price'] * chunk['qty']).fillna(0.0).astype('float32')\n",
    "        # vectorized groupby per chunk\n",
    "        grp = chunk.groupby('bin', sort=False)\n",
    "        qty_sum    = grp['qty'].sum(min_count=1)\n",
    "        dollar_sum = grp['px_qty'].sum(min_count=1)\n",
    "        trades_cnt = grp.size().astype('float32')\n",
    "        vwap       = dollar_sum / (qty_sum.replace(0, np.nan))\n",
    "        # OFI without apply\n",
    "        buy_qty  = chunk.loc[~chunk['is_buyer_maker'], 'qty'].groupby(chunk['bin']).sum(min_count=1)\n",
    "        sell_qty = chunk.loc[ chunk['is_buyer_maker'], 'qty'].groupby(chunk['bin']).sum(min_count=1)\n",
    "        ofi      = (buy_qty - sell_qty) / (qty_sum + 1e-9)\n",
    "        out = pd.DataFrame({\n",
    "            'trades_count': trades_cnt.astype('float32'),\n",
    "            'qty_sum':      qty_sum.astype('float32'),\n",
    "            'dollar_sum':   dollar_sum.astype('float32'),\n",
    "            'vwap':         vwap.astype('float32'),\n",
    "            'buy_qty':      buy_qty.astype('float32'),\n",
    "            'sell_qty':     sell_qty.astype('float32'),\n",
    "            'ofi':          ofi.astype('float32'),\n",
    "        })\n",
    "        acc = out if acc is None else acc.add(out, fill_value=0)\n",
    "        del chunk, grp, out; gc.collect()\n",
    "    return acc if acc is not None else pd.DataFrame()\n",
    "\n",
    "def aggregate_aggtrades_file_streaming(path):\n",
    "    acc = None\n",
    "    for chunk in stream_parquet_batches(path, columns=['price','quantity','transact_time']):\n",
    "        if chunk.empty: continue\n",
    "        tcol = detect_time_col(chunk, ('transact_time','time','timestamp')) or 'transact_time'\n",
    "        chunk[tcol] = pd.to_datetime(chunk[tcol], utc=True, errors='coerce')\n",
    "        chunk = chunk.dropna(subset=[tcol]).set_index(tcol)\n",
    "        chunk.rename(columns={'quantity':'qty'}, inplace=True)\n",
    "        chunk = _ensure_float32(chunk, ['price','qty'])\n",
    "        chunk['bin'] = chunk.index.floor(BAR_FREQ)\n",
    "        chunk['px_qty'] = (chunk['price'] * chunk['qty']).fillna(0.0).astype('float32')\n",
    "        grp = chunk.groupby('bin', sort=False)\n",
    "        qty_sum    = grp['qty'].sum(min_count=1)\n",
    "        dollar_sum = grp['px_qty'].sum(min_count=1)\n",
    "        agg_count  = grp.size().astype('float32')\n",
    "        vwap       = dollar_sum / (qty_sum.replace(0, np.nan))\n",
    "        out = pd.DataFrame({\n",
    "            'agg_count':      agg_count,\n",
    "            'agg_qty_sum':    qty_sum.astype('float32'),\n",
    "            'agg_dollar_sum': dollar_sum.astype('float32'),\n",
    "            'agg_vwap':       vwap.astype('float32'),\n",
    "        })\n",
    "        acc = out if acc is None else acc.add(out, fill_value=0)\n",
    "        del chunk, grp, out; gc.collect()\n",
    "    return acc if acc is not None else pd.DataFrame()\n",
    "\n",
    "def aggregate_bookdepth_file_streaming(path):\n",
    "    acc = None\n",
    "    for chunk in stream_parquet_batches(path, columns=['timestamp','percentage','depth','notional']):\n",
    "        if chunk.empty: continue\n",
    "        tcol = detect_time_col(chunk, ('timestamp','time')) or 'timestamp'\n",
    "        chunk[tcol] = pd.to_datetime(chunk[tcol], utc=True, errors='coerce')\n",
    "        chunk = chunk.dropna(subset=[tcol]).set_index(tcol)\n",
    "        chunk = _ensure_float32(chunk, ['percentage','depth','notional'])\n",
    "        chunk['bin'] = chunk.index.floor(BAR_FREQ)\n",
    "        grp = chunk.groupby('bin', sort=False)\n",
    "        out = pd.DataFrame({\n",
    "            'bd_notional_sum': grp['notional'].sum(min_count=1).astype('float32'),\n",
    "            'bd_depth_sum':    grp['depth'].sum(min_count=1).astype('float32'),\n",
    "        })\n",
    "        acc = out if acc is None else acc.add(out, fill_value=0)\n",
    "        del chunk, grp, out; gc.collect()\n",
    "    return acc if acc is not None else pd.DataFrame()\n",
    "\n",
    "def cache_or_build(kind, path, builder_func):\n",
    "    cpath = _cache_path(kind, path)\n",
    "    if cpath.exists():\n",
    "        return pd.read_parquet(cpath)\n",
    "    df = builder_func(path)\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df.index.name = 'time'\n",
    "    df.sort_index(inplace=True)\n",
    "    df.to_parquet(cpath)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ec6ce",
   "metadata": {},
   "source": [
    "## 3) Load sources (stream + cache heavy files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb67d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light sources\n",
    "def load_concat(files, columns=None):\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f, columns=columns) if columns else pd.read_parquet(f)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print('Failed to load', f, e)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "# KLINES (base)\n",
    "kl_raw = load_concat(manifest['klines'], columns=['open_time','close_time','open','high','low','close','volume'])\n",
    "print('klines raw:', kl_raw.shape)\n",
    "kl_df = to_utc_index(kl_raw, preferred_time_cols=('open_time','time','timestamp','close_time'), reconstruct_from_close=True)\n",
    "keep = [c for c in ['open','high','low','close','volume'] if c in kl_df.columns]\n",
    "kl_df = kl_df[keep].astype('float32')\n",
    "print('klines aligned:', kl_df.shape)\n",
    "\n",
    "# MARK / INDEX / PREMIUM\n",
    "mk_df = to_utc_index(load_concat(manifest['markpriceklines'], columns=['open_time','close_time','open','high','low','close','volume']))\n",
    "ix_df = to_utc_index(load_concat(manifest['indexpriceklines'], columns=['open_time','close_time','open','high','low','close','volume']))\n",
    "pr_df = to_utc_index(load_concat(manifest['premiumindexklines'], columns=['open_time','close_time','open','high','low','close','volume']))\n",
    "def sel_bars(df):\n",
    "    return df[[c for c in ['open','high','low','close','volume'] if c in df.columns]].astype('float32') if not df.empty else df\n",
    "mark_k, index_k, prem_k = sel_bars(mk_df), sel_bars(ix_df), sel_bars(pr_df)\n",
    "\n",
    "# METRICS (ffill to 15min)\n",
    "mt_raw = load_concat(manifest['metrics'], columns=['create_time','sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio'])\n",
    "mt_df  = to_utc_index(mt_raw, preferred_time_cols=('create_time','time','timestamp'))\n",
    "mt_15  = mt_df[['sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio']].astype('float32').resample(BAR_FREQ).ffill() if not mt_df.empty else pd.DataFrame()\n",
    "\n",
    "# HEAVY SOURCES — STREAM & CACHE PER FILE\n",
    "tr_list = manifest['trades']\n",
    "ag_list = manifest['aggtrades']\n",
    "bd_list = manifest['bookdepth']\n",
    "\n",
    "def process_list(kind, files, builder):\n",
    "    outs = []\n",
    "    for p in files:\n",
    "        out = cache_or_build(kind, p, builder)\n",
    "        if not out.empty:\n",
    "            outs.append(out)\n",
    "    if not outs:\n",
    "        return pd.DataFrame()\n",
    "    df_all = pd.concat(outs).sort_index()\n",
    "    return df_all.groupby(df_all.index).sum()\n",
    "\n",
    "tr_15 = process_list('trades', tr_list, aggregate_trades_file_streaming)\n",
    "ag_15 = process_list('aggtrades', ag_list, aggregate_aggtrades_file_streaming)\n",
    "bd_15 = process_list('bookdepth', bd_list, aggregate_bookdepth_file_streaming)\n",
    "\n",
    "print('Shapes — kl:', kl_df.shape,\n",
    "      'mark:', getattr(mark_k,'shape',()), 'index:', getattr(index_k,'shape',()), 'prem:', getattr(prem_k,'shape',()))\n",
    "print('Shapes — metrics15:', mt_15.shape, 'trades15:', tr_15.shape, 'aggtrades15:', ag_15.shape, 'bookdepth15:', bd_15.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa253892",
   "metadata": {},
   "source": [
    "## 4) Merge unified 15min base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = kl_df.copy()\n",
    "\n",
    "def safe_join(left, right):\n",
    "    return left.join(right, how='left') if not right.empty else left\n",
    "\n",
    "if not mark_k.empty:  base = safe_join(base, mark_k[['close']].rename(columns={'close':'mark_close'}))\n",
    "if not index_k.empty: base = safe_join(base, index_k[['close']].rename(columns={'close':'index_close'}))\n",
    "if not prem_k.empty:  base = safe_join(base, prem_k[['close']].rename(columns={'close':'premium_close'}))\n",
    "base = safe_join(base, mt_15)\n",
    "base = safe_join(base, tr_15)\n",
    "base = safe_join(base, ag_15)\n",
    "base = safe_join(base, bd_15)\n",
    "\n",
    "base = base.sort_index().replace([np.inf,-np.inf], np.nan).fillna(method='ffill').fillna(0)\n",
    "print('Unified base shape:', base.shape)\n",
    "base.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfbbab",
   "metadata": {},
   "source": [
    "## 5) Labels — Triple‑Barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HORIZON = 16; K_UP = 1.5; K_DN = 1.5; ATR_LEN = 14\n",
    "\n",
    "def compute_atr(df, atr_len=14):\n",
    "    high, low, close = df['high'], df['low'], df['close']\n",
    "    tr = pd.concat([(high-low).abs(), (high-close.shift()).abs(), (low-close.shift()).abs()], axis=1).max(axis=1)\n",
    "    return tr.ewm(span=atr_len, adjust=False).mean()\n",
    "\n",
    "def triple_barrier_labels(df, n=16, k_up=1.5, k_dn=1.5, atr_len=14):\n",
    "    atr = compute_atr(df, atr_len=atr_len)\n",
    "    price = df['close'].values\n",
    "    up_m = (k_up * atr / df['close']).fillna(method='bfill').values\n",
    "    dn_m = (k_dn * atr / df['close']).fillna(method='bfill').values\n",
    "    y = np.full(len(df), 2, dtype=np.int8)\n",
    "    hi, lo = df['high'].values, df['low'].values\n",
    "    L = len(df)\n",
    "    for i in range(max(0, L-n)):\n",
    "        p0 = price[i]\n",
    "        up = p0 * (1 + up_m[i]); dn = p0 * (1 - dn_m[i])\n",
    "        hp = hi[i+1:i+n+1]; lp = lo[i+1:i+n+1]\n",
    "        iu = np.where(hp >= up)[0]; idn = np.where(lp <= dn)[0]\n",
    "        if iu.size and (not idn.size or iu[0] < idn[0]): y[i] = 1\n",
    "        elif idn.size and (not iu.size or idn[0] < iu[0]): y[i] = 0\n",
    "        else: y[i] = 2\n",
    "    labels = pd.Series(y, index=df.index, name='label'); labels.iloc[-n:] = 2\n",
    "    return labels\n",
    "\n",
    "labels = triple_barrier_labels(base, n=N_HORIZON, k_up=K_UP, k_dn=K_DN, atr_len=ATR_LEN)\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b4a72",
   "metadata": {},
   "source": [
    "## 6) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X['ret_1'] = df['close'].pct_change()\n",
    "    for lag in [2,4,8,16,32,64]:\n",
    "        X[f'ret_{lag}'] = df['close'].pct_change(lag)\n",
    "    atr14 = compute_atr(df, atr_len=14)\n",
    "    X['atr14p'] = atr14/df['close']\n",
    "    ema8 = df['close'].ewm(span=8).mean(); ema21 = df['close'].ewm(span=21).mean()\n",
    "    X['ema_diff'] = ema8 - ema21\n",
    "    delta = df['close'].diff(); up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    rs = up.ewm(span=14).mean() / (down.ewm(span=14).mean() + 1e-9)\n",
    "    X['rsi14'] = 100 - 100/(1+rs)\n",
    "    if 'mark_close' in df.columns:  X['mark_spread_p']  = (df['mark_close']  - df['close'])/df['close']\n",
    "    if 'index_close' in df.columns: X['index_spread_p'] = (df['index_close'] - df['close'])/df['close']\n",
    "    if 'premium_close' in df.columns: X['premium_chg'] = df['premium_close'].pct_change()\n",
    "    if 'sum_open_interest' in df.columns: X['oi_chg_p'] = df['sum_open_interest'].pct_change().fillna(0)\n",
    "    if 'sum_taker_long_short_vol_ratio' in df.columns: X['taker_ls_chg_p'] = df['sum_taker_long_short_vol_ratio'].pct_change().fillna(0)\n",
    "    if 'ofi' in df.columns: X['ofi'] = df['ofi']\n",
    "    if 'trades_count' in df.columns: X['trades_count'] = df['trades_count']\n",
    "    if 'vwap' in df.columns: X['vwap_close_spread'] = (df['vwap'] - df['close'])/df['close']\n",
    "    if 'bd_notional_sum' in df.columns: X['bd_notional_sum'] = df['bd_notional_sum']\n",
    "    X = X.replace([np.inf,-np.inf], np.nan).fillna(method='bfill').fillna(0)\n",
    "    return X.astype('float32')\n",
    "\n",
    "X = engineer_features(base)\n",
    "print('X shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92cf4f",
   "metadata": {},
   "source": [
    "## 7) Train LightGBM + Isotonic calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d824e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "n = len(X); cal_size = max(int(n * 0.15), 500); split_idx = n - cal_size\n",
    "X_tr, y_tr = X.iloc[:split_idx], labels.iloc[:split_idx]\n",
    "X_cal, y_cal = X.iloc[split_idx:], labels.iloc[split_idx:]\n",
    "\n",
    "le = LabelEncoder(); y_tr_enc = le.fit_transform(y_tr.values); y_cal_enc = le.transform(y_cal.values)\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    objective='multiclass', num_class=3,\n",
    "    learning_rate=0.03, n_estimators=4000,\n",
    "    num_leaves=64, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    ")\n",
    "clf.fit(\n",
    "    X_tr, y_tr_enc,\n",
    "    eval_set=[(X_cal, y_cal_enc)], eval_metric='multi_logloss',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    ")\n",
    "\n",
    "cal = CalibratedClassifierCV(clf, method='isotonic', cv='prefit')\n",
    "cal.fit(X_cal, y_cal_enc)\n",
    "\n",
    "probs_cal = cal.predict_proba(X_cal)\n",
    "up_idx = list(le.classes_).index(1) if 1 in le.classes_ else 0\n",
    "print('Calibration Brier (UP on holdout):', brier_score_loss((y_cal_enc==1).astype(int), probs_cal[:, up_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0da24",
   "metadata": {},
   "source": [
    "## 8) Save artifacts & predict helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb542c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARTIFACT_DIR/'lgbm_calibrated.pkl', 'wb') as f: pickle.dump(cal, f)\n",
    "with open(ARTIFACT_DIR/'label_encoder.pkl', 'wb') as f: pickle.dump(le, f)\n",
    "with open(ARTIFACT_DIR/'feature_columns.json', 'w') as f: f.write(json.dumps(list(X.columns)))\n",
    "print('Saved artifacts to', ARTIFACT_DIR)\n",
    "\n",
    "def predict_live(latest_row: pd.Series, calibrated_model, label_encoder, feature_columns):\n",
    "    Xr = latest_row[feature_columns].values.reshape(1,-1)\n",
    "    proba = calibrated_model.predict_proba(Xr)[0]\n",
    "    classes = list(label_encoder.classes_)\n",
    "    out = {}\n",
    "    for cname, cid in [('P_down',0), ('P_up',1), ('P_neutral',2)]:\n",
    "        out[cname] = float(proba[classes.index(cid)]) if cid in classes else np.nan\n",
    "    return out\n",
    "\n",
    "print('Latest probabilities:', predict_live(X.iloc[-1], cal, le, X.columns))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
