# Create a new memory-efficient Colab notebook per user specs:
# - Inputs: 15m candle CLOSE only, order book depth (bookDepth), "large resting orders" proxy, and all metrics.
# - Create OI RSI(16) on 15m OI.
# - Use a Transformer model for up/down next-N prediction.
# - Stream + cache bookDepth per-file with pyarrow.
# - Save to /mnt/data/binance_transformer_15m_depth_metrics.ipynb

import nbformat as nbf

nb = nbf.v4.new_notebook()
cells = []

cells.append(nbf.v4.new_markdown_cell(
"# Colab: Transformer for 15‑min Direction — Close, Order Book Depth, Metrics (+ OI RSI16)\n"
"\n"
"**Data used:**\n"
"- 15‑min **candle close** (from `klines`)\n"
"- **Order book depth** & a proxy for **large resting limit orders** (from `bookDepth`)\n"
"- **All metrics** (from `metrics`) + **OI RSI(16)** computed on 15‑min open interest\n"
"\n"
"**Model:** Transformer encoder → binary classification P(up in next *N* bars).\n"
"\n"
"**Memory Safety:** Book depth is streamed in chunks with PyArrow and cached to tiny 15‑min aggregates.\n"
))

# Env pin + restart cell
cells.append(nbf.v4.new_markdown_cell("## 0) Pin deps (run ONCE), then Runtime → Restart"))
cells.append(nbf.v4.new_code_cell(
"# Run this cell ONCE. It will restart the Python process to fix any PyArrow binary mismatch.\n"
"%pip uninstall -y pyarrow -q\n"
"%pip install -q pandas==2.2.2 pyarrow==16.1.0 fastparquet==2024.5.0 torch==2.3.1 scikit-learn==1.5.1\n"
"import os, importlib\n"
"import pandas, pyarrow\n"
"print('Pinned:', pandas.__version__, pyarrow.__version__)\n"
"os.kill(os.getpid(), 9)"
))

# Setup
cells.append(nbf.v4.new_markdown_cell("## 1) Setup & Mount Drive"))
cells.append(nbf.v4.new_code_cell(
"import os, sys, json, gc, math, pickle\n"
"from pathlib import Path\n"
"import numpy as np\n"
"import pandas as pd\n"
"from google.colab import drive\n"
"drive.mount('/content/drive')\n"
"DATA_ROOT = Path('/content/drive/MyDrive/binance_data')\n"
"CACHE_DIR = Path('/content/drive/MyDrive/binance_cache')\n"
"ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n"
"CACHE_DIR.mkdir(parents=True, exist_ok=True)\n"
"ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n"
"assert DATA_ROOT.exists(), f'Missing: {DATA_ROOT}'\n"
"BAR_FREQ = '15min'"
))

# Manifest
cells.append(nbf.v4.new_markdown_cell("## 2) Discover files"))
cells.append(nbf.v4.new_code_cell(
"def build_manifest(root: Path) -> dict:\n"
"    m = {k: [] for k in ['klines','metrics','bookdepth','other']}\n"
"    for p in root.rglob('*.parquet'):\n"
"        name = p.name.lower()\n"
"        if 'kline' in name:\n"
"            m['klines'].append(str(p))\n"
"        elif 'metric' in name or 'open_interest' in name:\n"
"            m['metrics'].append(str(p))\n"
"        elif 'bookdepth' in name or 'orderbook' in name:\n"
"            m['bookdepth'].append(str(p))\n"
"        else:\n"
"            m['other'].append(str(p))\n"
"    for k in m:\n"
"        m[k] = sorted(m[k])\n"
"    return m\n"
"\n"
"manifest = build_manifest(DATA_ROOT)\n"
"for k,v in manifest.items():\n"
"    print(f\"{k}: {len(v)} files\")"
))

# Helpers
cells.append(nbf.v4.new_markdown_cell("## 3) Helpers — time handling, loading & streaming"))
cells.append(nbf.v4.new_code_cell(
"import pyarrow.parquet as pq\n"
"\n"
"def _cache_path(kind: str, src_path: str) -> Path:\n"
"    fn = Path(src_path).name.replace('.parquet', f'.{kind}.15m.parquet')\n"
"    return CACHE_DIR / fn\n"
"\n"
"def detect_time_col(df, candidates):\n"
"    for c in candidates:\n"
"        if c in df.columns: return c\n"
"    return None\n"
"\n"
"def to_utc_index(\n"
"    df,\n"
"    preferred_time_cols=('open_time','time','timestamp','create_time','transact_time','close_time'),\n"
"    reconstruct_from_close=False\n"
"):\n"
"    if df.empty:\n"
"        return df\n"
"    def _detect(df, cands):\n"
"        for c in cands:\n"
"            if c in df.columns:\n"
"                return c\n"
"        return None\n"
"    tcol = _detect(df, preferred_time_cols)\n"
"    if reconstruct_from_close and (tcol is None or tcol == 'close_time'):\n"
"        if 'close_time' in df.columns:\n"
"            close_ts = pd.to_datetime(df['close_time'], errors='coerce', utc=True)\n"
"            df = df.copy(); df['open_time'] = close_ts - pd.Timedelta('15min'); tcol = 'open_time'\n"
"        else:\n"
"            raise ValueError('Need close_time to reconstruct open_time.')\n"
"    if tcol is None:\n"
"        raise AssertionError(f'Expected one of {preferred_time_cols}, got {list(df.columns)[:10]} ...')\n"
"    df = df.copy()\n"
"    df[tcol] = pd.to_datetime(df[tcol], errors='coerce', utc=False)\n"
"    df = df.dropna(subset=[tcol]).sort_values(tcol).drop_duplicates(tcol)\n"
"    df = df.set_index(tcol)\n"
"    if not isinstance(df.index, pd.DatetimeIndex):\n"
"        raise TypeError(f'Index is not DatetimeIndex after setting {tcol}')\n"
"    df.index = df.index.tz_localize('UTC') if df.index.tz is None else df.index.tz_convert('UTC')\n"
"    return df\n"
"\n"
"def load_concat(files, columns=None):\n"
"    dfs = []\n"
"    for f in files:\n"
"        try:\n"
"            df = pd.read_parquet(f, columns=columns, engine='pyarrow') if columns else pd.read_parquet(f, engine='pyarrow')\n"
"            dfs.append(df)\n"
"        except Exception as e:\n"
"            print('Failed to load', f, e)\n"
"    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n"
"\n"
"def stream_parquet_batches(path, columns=None, batch_size=200_000):\n"
"    pf = pq.ParquetFile(path)\n"
"    for batch in pf.iter_batches(batch_size=batch_size, columns=columns):\n"
"        yield batch.to_pandas()\n"
"\n"
"def _ensure_float32(df, cols):\n"
"    for c in cols:\n"
"        if c in df.columns:\n"
"            df[c] = pd.to_numeric(df[c], errors='coerce').astype('float32')\n"
"    return df\n"
))

# BookDepth streaming aggregator
cells.append(nbf.v4.new_markdown_cell("## 4) Stream & cache BookDepth → 15m features"))
cells.append(nbf.v4.new_code_cell(
"def aggregate_bookdepth_file_streaming(path):\n"
"    outs = []\n"
"    for chunk in stream_parquet_batches(path, columns=['timestamp','percentage','depth','notional']):\n"
"        if chunk.empty:\n"
"            continue\n"
"        tcol = detect_time_col(chunk, ('timestamp','time')) or 'timestamp'\n"
"        chunk[tcol] = pd.to_datetime(chunk[tcol], utc=True, errors='coerce')\n"
"        chunk = chunk.dropna(subset=[tcol]).set_index(tcol)\n"
"        chunk = _ensure_float32(chunk, ['percentage','depth','notional'])\n"
"        bins = chunk.index.floor('15min')\n"
"        # Core depth features\n"
"        bd_notional_sum = chunk['notional'].groupby(bins).sum(min_count=1)\n"
"        bd_depth_sum    = chunk['depth'].groupby(bins).sum(min_count=1)\n"
"        # Large resting orders proxy: max notional per 15m\n"
"        bd_notional_max = chunk['notional'].groupby(bins).max()\n"
"        out = pd.DataFrame({\n"
"            'bd_notional_sum': bd_notional_sum.astype('float32'),\n"
"            'bd_depth_sum':    bd_depth_sum.astype('float32'),\n"
"            'bd_notional_max': bd_notional_max.astype('float32'),\n"
"        })\n"
"        out.index.name = 'time'\n"
"        outs.append(out)\n"
"        del chunk, out\n"
"    if not outs:\n"
"        return pd.DataFrame()\n"
"    df_all = pd.concat(outs)\n"
"    return df_all.groupby(level=0, sort=True).sum()\n"
"\n"
"def process_bookdepth(files):\n"
"    outs = []\n"
"    for p in files:\n"
"        cpath = _cache_path('bookdepth', p)\n"
"        if cpath.exists():\n"
"            out = pd.read_parquet(cpath)\n"
"        else:\n"
"            out = aggregate_bookdepth_file_streaming(p)\n"
"            if out is not None and not out.empty:\n"
"                out.sort_index(inplace=True)\n"
"                out.to_parquet(cpath)\n"
"        if out is not None and not out.empty:\n"
"            outs.append(out)\n"
"    if not outs:\n"
"        return pd.DataFrame()\n"
"    df_all = pd.concat(outs)\n"
"    return df_all.groupby(level=0, sort=True).sum()"
))

# Load klines & metrics, compute OI RSI16
cells.append(nbf.v4.new_markdown_cell("## 5) Load klines (close) & metrics (OI RSI16)"))
cells.append(nbf.v4.new_code_cell(
"# KLINES (close only)\n"
"kl_raw = load_concat(manifest['klines'], columns=['open_time','close_time','close'])\n"
"print('klines raw:', kl_raw.shape)\n"
"kl_df = to_utc_index(kl_raw, preferred_time_cols=('open_time','time','timestamp','close_time'), reconstruct_from_close=True)\n"
"kl_df = kl_df[['close']].astype('float32')\n"
"print('klines aligned:', kl_df.shape)\n"
"\n"
"# METRICS\n"
"mt_raw = load_concat(manifest['metrics'], columns=['create_time','sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio'])\n"
"mt_df  = to_utc_index(mt_raw, preferred_time_cols=('create_time','time','timestamp'))\n"
"if not mt_df.empty:\n"
"    mt_15 = mt_df[['sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio']].astype('float32').resample('15min').ffill()\n"
"else:\n"
"    mt_15 = pd.DataFrame()\n"
"print('metrics 15m:', mt_15.shape)\n"
"\n"
"def rsi_series(x: pd.Series, length=16):\n"
"    delta = x.diff()\n"
"    up = delta.clip(lower=0)\n"
"    down = -delta.clip(upper=0)\n"
"    roll_up = up.ewm(alpha=1/length, adjust=False).mean()\n"
"    roll_down = down.ewm(alpha=1/length, adjust=False).mean()\n"
"    rs = roll_up / (roll_down + 1e-9)\n"
"    return 100 - (100 / (1 + rs))\n"
"\n"
"if not mt_15.empty and 'sum_open_interest' in mt_15.columns:\n"
"    mt_15['oi_rsi16'] = rsi_series(mt_15['sum_open_interest'].astype('float32'), length=16).astype('float32')\n"
"else:\n"
"    mt_15['oi_rsi16'] = np.nan"
))

# Build bookdepth
cells.append(nbf.v4.new_markdown_cell("## 6) Build BookDepth aggregates (streamed)"))
cells.append(nbf.v4.new_code_cell(
"bd_15 = process_bookdepth(manifest['bookdepth'])\n"
"print('bookdepth15:', bd_15.shape)\n"
"bd_15.tail()"
))

# Merge base
cells.append(nbf.v4.new_markdown_cell("## 7) Merge to a unified 15m frame"))
cells.append(nbf.v4.new_code_cell(
"base = kl_df.copy()\n"
"def safe_join(left, right):\n"
"    return left.join(right, how='left') if right is not None and not right.empty else left\n"
"\n"
"base = safe_join(base, mt_15)\n"
"base = safe_join(base, bd_15)\n"
"base = base.sort_index().replace([np.inf,-np.inf], np.nan).fillna(method='ffill').dropna()\n"
"print('Unified base shape:', base.shape)\n"
"base.tail()"
))

# Features & labels
cells.append(nbf.v4.new_markdown_cell("## 8) Features (close, depth, metrics) & Binary labels"))
cells.append(nbf.v4.new_code_cell(
"N_HORIZON = 16   # predict direction over next N 15m bars (~4h)\n"
"NEUTRAL_EPS = 0.0  # set >0 to ignore tiny moves\n"
"\n"
"# Features\n"
"feat_cols = ['close','sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio','oi_rsi16','bd_notional_sum','bd_depth_sum','bd_notional_max']\n"
"X = base[feat_cols].copy()\n"
"# Add a stationary helper: close return\n"
"X['close_ret_1'] = X['close'].pct_change().fillna(0).astype('float32')\n"
"\n"
"# Labels (binary up/down)\n"
"future_close = base['close'].shift(-N_HORIZON)\n"
"fwd_ret = (future_close - base['close']) / base['close']\n"
"y = (fwd_ret > NEUTRAL_EPS).astype('int8')\n"
"\n"
"df = X.copy()\n"
"df['target'] = y\n"
"df = df.dropna()\n"
"print('Final dataset:', df.shape)\n"
"df.tail()"
))

# Train/Val/Test split
cells.append(nbf.v4.new_markdown_cell("## 9) Time split & sequence dataset (Transformer)"))
cells.append(nbf.v4.new_code_cell(
"from sklearn.preprocessing import StandardScaler\n"
"\n"
"LOOKBACK = 128  # timesteps per sample (~32 hours)\n"
"STRIDE = 1\n"
"\n"
"# time-based split 70/15/15\n"
"n = len(df)\n"
"i_tr = int(n*0.7)\n"
"i_va = int(n*0.85)\n"
"train_df = df.iloc[:i_tr]\n"
"val_df   = df.iloc[i_tr:i_va]\n"
"test_df  = df.iloc[i_va:]\n"
"\n"
"features = [c for c in df.columns if c != 'target']\n"
"scaler = StandardScaler()\n"
"scaler.fit(train_df[features])\n"
"\n"
"def make_sequences(frame):\n"
"    Xn = scaler.transform(frame[features]).astype('float32')\n"
"    y  = frame['target'].astype('int64').values\n"
"    X_seq, y_seq = [], []\n"
"    for t in range(LOOKBACK, len(frame), STRIDE):\n"
"        X_seq.append(Xn[t-LOOKBACK:t, :])\n"
"        y_seq.append(y[t])\n"
"    return np.stack(X_seq), np.array(y_seq)\n"
"\n"
"Xtr, Ytr = make_sequences(train_df)\n"
"Xva, Yva = make_sequences(val_df)\n"
"Xte, Yte = make_sequences(test_df)\n"
"print('Seq shapes:', Xtr.shape, Xva.shape, Xte.shape)"
))

# Transformer model and training
cells.append(nbf.v4.new_markdown_cell("## 10) Transformer model & training"))
cells.append(nbf.v4.new_code_cell(
"import torch\n"
"import torch.nn as nn\n"
"from torch.utils.data import TensorDataset, DataLoader\n"
"\n"
"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
"\n"
"class PositionalEncoding(nn.Module):\n"
"    def __init__(self, d_model, max_len=10000):\n"
"        super().__init__()\n"
"        pe = torch.zeros(max_len, d_model)\n"
"        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n"
"        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n"
"        pe[:, 0::2] = torch.sin(position * div_term)\n"
"        pe[:, 1::2] = torch.cos(position * div_term)\n"
"        self.register_buffer('pe', pe.unsqueeze(0))\n"
"    def forward(self, x):\n"
"        L = x.size(1)\n"
"        return x + self.pe[:, :L, :]\n"
"\n"
"class TimeSeriesTransformer(nn.Module):\n"
"    def __init__(self, num_features, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1):\n"
"        super().__init__()\n"
"        self.input_proj = nn.Linear(num_features, d_model)\n"
"        self.posenc = PositionalEncoding(d_model)\n"
"        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n"
"        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n"
"        self.head = nn.Sequential(\n"
"            nn.LayerNorm(d_model),\n"
"            nn.Linear(d_model, 1)\n"
"        )\n"
"    def forward(self, x):\n"
"        z = self.input_proj(x)\n"
"        z = self.posenc(z)\n"
"        z = self.encoder(z)\n"
"        # use last token representation\n"
"        z_last = z[:, -1, :]\n"
"        logits = self.head(z_last).squeeze(-1)\n"
"        return logits\n"
"\n"
"num_features = len(features)\n"
"model = TimeSeriesTransformer(num_features=num_features).to(device)\n"
"\n"
"train_ds = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(Ytr))\n"
"val_ds   = TensorDataset(torch.from_numpy(Xva), torch.from_numpy(Yva))\n"
"test_ds  = TensorDataset(torch.from_numpy(Xte), torch.from_numpy(Yte))\n"
"\n"
"train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=True)\n"
"val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False)\n"
"\n"
"criterion = nn.BCEWithLogitsLoss()\n"
"optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n"
"\n"
"def evaluate(loader):\n"
"    model.eval(); loss_sum=0; n=0; correct=0\n"
"    with torch.no_grad():\n"
"        for xb,yb in loader:\n"
"            xb = xb.to(device)\n"
"            yb = yb.float().to(device)\n"
"            logits = model(xb)\n"
"            loss = criterion(logits, yb)\n"
"            loss_sum += loss.item()*len(xb)\n"
"            preds = (torch.sigmoid(logits) > 0.5).long()\n"
"            correct += (preds.cpu()==yb.long().cpu()).sum().item()\n"
"            n += len(xb)\n"
"    return loss_sum/n, correct/n\n"
"\n"
"BEST_PATH = ARTIFACT_DIR/'transformer_close_depth_metrics.pth'\n"
"best_val = float('inf')\n"
"epochs = 12\n"
"for ep in range(1, epochs+1):\n"
"    model.train()\n"
"    for xb,yb in train_loader:\n"
"        xb = xb.to(device); yb = yb.float().to(device)\n"
"        optimizer.zero_grad(); logits = model(xb)\n"
"        loss = criterion(logits, yb)\n"
"        loss.backward(); optimizer.step()\n"
"    vloss, vacc = evaluate(val_loader)\n"
"    print(f'Epoch {ep:02d}  val_loss={vloss:.4f}  val_acc={vacc:.3f}')\n"
"    if vloss < best_val:\n"
"        best_val = vloss\n"
"        torch.save({'model_state': model.state_dict(), 'features': features, 'scaler': scaler, 'config': {'lookback': LOOKBACK, 'horizon': N_HORIZON}}, BEST_PATH)\n"
"        print('  ✓ saved best')\n"
))

# Test & save artifacts
cells.append(nbf.v4.new_markdown_cell("## 11) Evaluate on test & export artifacts"))
cells.append(nbf.v4.new_code_cell(
"test_loader = DataLoader(test_ds, batch_size=512, shuffle=False)\n"
"tloss, tacc = evaluate(test_loader)\n"
"print('Test  loss:', round(tloss,4), ' acc:', round(tacc,3))\n"
"\n"
"import joblib\n"
"joblib.dump(scaler, ARTIFACT_DIR/'scaler_close_depth_metrics.gz')\n"
"with open(ARTIFACT_DIR/'features_close_depth_metrics.json','w') as f:\n"
"    json.dump(features, f)\n"
"print('Artifacts in', ARTIFACT_DIR)"
))

# Inference helper
cells.append(nbf.v4.new_markdown_cell("## 12) Inference helper (latest window → P(up))"))
cells.append(nbf.v4.new_code_cell(
"def predict_latest_prob_up(model_path=BEST_PATH):\n"
"    ckpt = torch.load(model_path, map_location=device)\n"
"    feats = ckpt['features']\n"
"    lookback = ckpt['config']['lookback']\n"
"    scaler2 = scaler  # using the fitted scaler from this run\n"
"    # take the last `lookback` rows from current df\n"
"    window = df[feats].iloc[-lookback:]\n"
"    Xw = scaler2.transform(window).astype('float32')\n"
"    xb = torch.from_numpy(Xw).unsqueeze(0).to(device)\n"
"    m = TimeSeriesTransformer(num_features=len(feats)).to(device)\n"
"    m.load_state_dict(ckpt['model_state']); m.eval()\n"
"    with torch.no_grad():\n"
"        logit = m(xb)\n"
"        p_up = torch.sigmoid(logit).item()\n"
"    return p_up\n"
"\n"
"print('P(up) next N bars:', round(predict_latest_prob_up(), 4))"
))

nb['cells'] = cells
path = '/mnt/data/binance_transformer_15m_depth_metrics.ipynb'
with open(path, 'w', encoding='utf-8') as f:
    nbf.write(nb, f)

path
