{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5819a59",
   "metadata": {},
   "source": [
    "# Colab: Binance Futures Direction Model (LightGBM Only, Full Data)\n",
    "\n",
    "**What this notebook does**\n",
    "- Mounts **Google Drive**; scans `/content/drive/MyDrive/binance_data`\n",
    "- Loads ALL folders you provided (klines, mark/index/premium klines, metrics, trades, aggTrades, bookDepth)\n",
    "- Robust timestamp parsing:\n",
    "  - Uses common time columns per dataset\n",
    "  - Reconstructs `open_time` from `close_time` for klines if needed\n",
    "- Aggregates non-bar datasets to **15m**\n",
    "- Builds a unified features frame\n",
    "- Labels with **triple-barrier** (UP/DOWN/NEUTRAL) for next `n` periods\n",
    "- Trains **LightGBM** (multiclass) and calibrates probabilities via **isotonic**\n",
    "- Saves model + feature schema to Drive and provides a **predict()** stub for live usage\n",
    "\n",
    "> Adjust `DATA_ROOT`, `BAR_FREQ`, and labeling params to your liking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510f4d5",
   "metadata": {},
   "source": [
    "## 0) Setup & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install lightgbm==4.3.0 pyarrow==16.1.0 fastparquet==2024.5.0\n",
    "import os, sys, json, math, gc, pickle, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7a5b1",
   "metadata": {},
   "source": [
    "## 1) Configure paths & discover Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a41d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === Adjust if your data is elsewhere in Drive ===\n",
    "DATA_ROOT = Path('/content/drive/MyDrive/binance_data')\n",
    "assert DATA_ROOT.exists(), f'Data root not found: {DATA_ROOT}'\n",
    "\n",
    "def build_manifest(root: Path) -> dict:\n",
    "    manifest = {k: [] for k in [\n",
    "        'klines','markpriceklines','indexpriceklines','premiumindexklines',\n",
    "        'metrics','trades','aggtrades','bookdepth','other']}\n",
    "    for p in root.rglob('*.parquet'):\n",
    "        name = p.name.lower()\n",
    "        if 'markprice' in name and 'kline' in name:\n",
    "            manifest['markpriceklines'].append(str(p))\n",
    "        elif 'indexprice' in name and 'kline' in name:\n",
    "            manifest['indexpriceklines'].append(str(p))\n",
    "        elif 'premiumindex' in name and 'kline' in name:\n",
    "            manifest['premiumindexklines'].append(str(p))\n",
    "        elif 'kline' in name or 'klines' in name:\n",
    "            manifest['klines'].append(str(p))\n",
    "        elif 'aggtrade' in name:\n",
    "            manifest['aggtrades'].append(str(p))\n",
    "        elif 'bookdepth' in name or 'orderbook' in name:\n",
    "            manifest['bookdepth'].append(str(p))\n",
    "        elif 'metrics' in name or 'open_interest' in name:\n",
    "            manifest['metrics'].append(str(p))\n",
    "        elif 'trade' in name:\n",
    "            manifest['trades'].append(str(p))\n",
    "        else:\n",
    "            manifest['other'].append(str(p))\n",
    "    return manifest\n",
    "\n",
    "manifest = build_manifest(DATA_ROOT)\n",
    "for k,v in manifest.items():\n",
    "    print(f\"{k}: {len(v)} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c913e",
   "metadata": {},
   "source": [
    "## 2) Load helpers, robust time handling, and 15m aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global bar frequency ---\n",
    "BAR_FREQ = '15T'  # 15-minute bars\n",
    "\n",
    "def load_concat(files):\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print('Failed to load', f, e)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def detect_time_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def to_utc_index(\n",
    "    df,\n",
    "    preferred_time_cols=('open_time','time','timestamp','create_time','transact_time','close_time'),\n",
    "    reconstruct_from_close=False\n",
    "):\n",
    "    \"\"\"Parse the first matching time column to a UTC DatetimeIndex.\n",
    "    If reconstruct_from_close is True and only close_time exists, create open_time = close_time - BAR_FREQ.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    tcol = detect_time_col(df, preferred_time_cols)\n",
    "\n",
    "    # Special case: klines may only have close_time\n",
    "    if reconstruct_from_close and (tcol is None or tcol == 'close_time'):\n",
    "        if 'close_time' in df.columns:\n",
    "            close_ts = pd.to_datetime(df['close_time'], utc=True, errors='coerce')\n",
    "            open_ts = close_ts - pd.Timedelta(BAR_FREQ)\n",
    "            df = df.copy()\n",
    "            df['open_time'] = open_ts\n",
    "            tcol = 'open_time'\n",
    "        else:\n",
    "            raise ValueError(\"Need close_time to reconstruct open_time.\")\n",
    "\n",
    "    if tcol is None:\n",
    "        raise AssertionError(f\"Expected one of {preferred_time_cols}, got {list(df.columns)[:10]} ...\")\n",
    "\n",
    "    df[tcol] = pd.to_datetime(df[tcol], utc=True, errors='coerce')\n",
    "    df = df.dropna(subset=[tcol]).sort_values(tcol).drop_duplicates(tcol)\n",
    "    df = df.set_index(tcol)\n",
    "    if df.index.tz is None:\n",
    "        df = df.tz_localize('UTC')\n",
    "    else:\n",
    "        df = df.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "# --- Aggregators to 15m ---\n",
    "def _to_bool_series(s):\n",
    "    \"\"\"Robust conversion of is_buyer_maker to bool.\"\"\"\n",
    "    if s.dtype == 'bool':\n",
    "        return s\n",
    "    mapping_true = {True, 'true', 'True', 1, '1'}\n",
    "    mapping_false = {False, 'false', 'False', 0, '0'}\n",
    "    return s.map(lambda x: True if x in mapping_true else (False if x in mapping_false else False)).astype(bool)\n",
    "\n",
    "def agg_trades_15m(df):\n",
    "    # trades: id, price, qty, quote_qty, time, is_buyer_maker\n",
    "    if df.empty: return df\n",
    "    df = df.copy()\n",
    "    for col in ['price','qty','quote_qty']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if 'is_buyer_maker' in df.columns:\n",
    "        df['is_buyer_maker'] = _to_bool_series(df['is_buyer_maker'])\n",
    "    df['dollar'] = (df.get('price',0) * df.get('qty',0)).fillna(0)\n",
    "    g = df.resample(BAR_FREQ)\n",
    "    out = pd.DataFrame({\n",
    "        'trades_count': g.size(),\n",
    "        'qty_sum': g['qty'].sum(min_count=1),\n",
    "        'dollar_sum': g['dollar'].sum(min_count=1),\n",
    "        'vwap': g.apply(lambda x: (x['price']*x['qty']).sum()/max(x['qty'].sum(),1e-9) if 'price' in x and 'qty' in x else np.nan)\n",
    "    })\n",
    "    if 'is_buyer_maker' in df.columns:\n",
    "        def side_sum(x, side_bool):\n",
    "            if len(x)==0: return 0.0\n",
    "            return x.loc[x['is_buyer_maker']==side_bool, 'qty'].sum()\n",
    "        out['buy_qty'] = g.apply(lambda x: side_sum(x, False))\n",
    "        out['sell_qty'] = g.apply(lambda x: side_sum(x, True))\n",
    "        out['ofi'] = (out['buy_qty'] - out['sell_qty']) / (out['qty_sum'] + 1e-9)\n",
    "    else:\n",
    "        out['buy_qty'] = 0.0; out['sell_qty'] = 0.0; out['ofi'] = 0.0\n",
    "    return out\n",
    "\n",
    "def agg_aggtrades_15m(df):\n",
    "    # aggtrades: agg_trade_id, price, quantity, ..., is_buyer_maker\n",
    "    if df.empty: return df\n",
    "    df = df.copy()\n",
    "    if 'quantity' in df.columns:\n",
    "        df.rename(columns={'quantity':'qty'}, inplace=True)\n",
    "    for col in ['price','qty']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['dollar'] = (df.get('price',0) * df.get('qty',0)).fillna(0)\n",
    "    g = df.resample(BAR_FREQ)\n",
    "    out = pd.DataFrame({\n",
    "        'agg_count': g.size(),\n",
    "        'agg_qty_sum': g['qty'].sum(min_count=1),\n",
    "        'agg_dollar_sum': g['dollar'].sum(min_count=1),\n",
    "        'agg_vwap': g.apply(lambda x: (x['price']*x['qty']).sum()/max(x['qty'].sum(),1e-9) if 'price' in x and 'qty' in x else np.nan)\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def agg_bookdepth_15m(df):\n",
    "    # bookdepth: timestamp, percentage, depth, notional (pre-aggregated snapshots)\n",
    "    if df.empty: return df\n",
    "    df = df.copy()\n",
    "    for col in ['percentage','depth','notional']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    g = df.resample(BAR_FREQ)\n",
    "    out = pd.DataFrame({\n",
    "        'bd_notional_sum': g['notional'].sum(min_count=1),\n",
    "        'bd_depth_sum': g['depth'].sum(min_count=1)\n",
    "    })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9060b",
   "metadata": {},
   "source": [
    "## 3) Load datasets and align to 15m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- klines (base OHLCV 15m) ---\n",
    "kl_raw = load_concat(manifest['klines'])\n",
    "print('klines raw:', kl_raw.shape)\n",
    "kl_df = to_utc_index(kl_raw, preferred_time_cols=('open_time','time','timestamp','close_time'), reconstruct_from_close=True)\n",
    "keep_cols = [c for c in ['open','high','low','close','volume'] if c in kl_df.columns]\n",
    "assert len(keep_cols) >= 4, 'Expected at least open/high/low/close in klines.'\n",
    "kl_df = kl_df[keep_cols].astype('float32')\n",
    "print('klines aligned:', kl_df.shape)\n",
    "\n",
    "# --- mark/index/premium klines ---\n",
    "mk_df = to_utc_index(load_concat(manifest['markpriceklines']))\n",
    "ix_df = to_utc_index(load_concat(manifest['indexpriceklines']))\n",
    "pr_df = to_utc_index(load_concat(manifest['premiumindexklines']))\n",
    "def sel_bars(df):\n",
    "    return df[[c for c in ['open','high','low','close','volume'] if c in df.columns]].astype('float32') if not df.empty else df\n",
    "mark_k = sel_bars(mk_df)\n",
    "index_k = sel_bars(ix_df)\n",
    "prem_k = sel_bars(pr_df)\n",
    "print('mark/index/premium:', mark_k.shape, index_k.shape, prem_k.shape)\n",
    "\n",
    "# --- metrics (ffill to 15m) ---\n",
    "mt_df = to_utc_index(load_concat(manifest['metrics']), preferred_time_cols=('create_time','time','timestamp'))\n",
    "if not mt_df.empty:\n",
    "    metrics_cols = [c for c in ['sum_open_interest','sum_open_interest_value','sum_toptrader_long_short_ratio','sum_taker_long_short_vol_ratio'] if c in mt_df.columns]\n",
    "    mt_15 = mt_df[metrics_cols].astype('float32').resample(BAR_FREQ).ffill()\n",
    "else:\n",
    "    mt_15 = pd.DataFrame()\n",
    "print('metrics 15m:', mt_15.shape)\n",
    "\n",
    "# --- trades & aggTrades (aggregate to 15m) ---\n",
    "tr_df = to_utc_index(load_concat(manifest['trades']), preferred_time_cols=('time','timestamp'))\n",
    "ag_df = to_utc_index(load_concat(manifest['aggtrades']), preferred_time_cols=('transact_time','time','timestamp'))\n",
    "tr_15 = agg_trades_15m(tr_df) if not tr_df.empty else pd.DataFrame()\n",
    "ag_15 = agg_aggtrades_15m(ag_df) if not ag_df.empty else pd.DataFrame()\n",
    "print('trades15 / aggtrades15:', tr_15.shape, ag_15.shape)\n",
    "\n",
    "# --- bookDepth (aggregate to 15m) ---\n",
    "bd_df = to_utc_index(load_concat(manifest['bookdepth']), preferred_time_cols=('timestamp','time'))\n",
    "bd_15 = agg_bookdepth_15m(bd_df) if not bd_df.empty else pd.DataFrame()\n",
    "print('bookdepth15:', bd_15.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b57f8",
   "metadata": {},
   "source": [
    "## 4) Merge into unified 15m feature base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22390a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = kl_df.copy()\n",
    "\n",
    "def safe_join(left, right):\n",
    "    return left.join(right, how='left') if not right.empty else left\n",
    "\n",
    "# spreads and premiums\n",
    "if not mark_k.empty:\n",
    "    base = safe_join(base, mark_k[['close']].rename(columns={'close':'mark_close'}))\n",
    "if not index_k.empty:\n",
    "    base = safe_join(base, index_k[['close']].rename(columns={'close':'index_close'}))\n",
    "if not prem_k.empty:\n",
    "    base = safe_join(base, prem_k[['close']].rename(columns={'close':'premium_close'}))\n",
    "\n",
    "# metrics\n",
    "base = safe_join(base, mt_15)\n",
    "\n",
    "# order flow & depth\n",
    "base = safe_join(base, tr_15)\n",
    "base = safe_join(base, ag_15)\n",
    "base = safe_join(base, bd_15)\n",
    "\n",
    "base = base.sort_index().replace([np.inf,-np.inf], np.nan).fillna(method='ffill').fillna(0)\n",
    "print('Unified feature base shape:', base.shape)\n",
    "base.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0897fc2",
   "metadata": {},
   "source": [
    "## 5) Triple-barrier labels (next n periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label params (tune as desired)\n",
    "N_HORIZON = 16   # predict direction within next n bars (4 hours on 15m)\n",
    "K_UP = 1.5\n",
    "K_DN = 1.5\n",
    "ATR_LEN = 14\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, atr_len=14):\n",
    "    high = df['high']; low = df['low']; close = df['close']\n",
    "    tr1 = (high - low).abs()\n",
    "    tr2 = (high - close.shift()).abs()\n",
    "    tr3 = (low - close.shift()).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(span=atr_len, adjust=False).mean()\n",
    "    return atr\n",
    "\n",
    "def triple_barrier_labels(df, n=16, k_up=1.5, k_dn=1.5, atr_len=14):\n",
    "    atr = compute_atr(df, atr_len=atr_len)\n",
    "    price = df['close'].values\n",
    "    up_mult = (k_up * atr / df['close']).fillna(method='bfill').values\n",
    "    dn_mult = (k_dn * atr / df['close']).fillna(method='bfill').values\n",
    "    y = np.full(len(df), 2, dtype=np.int8)  # 1=UP,0=DOWN,2=NEUTRAL\n",
    "    highs = df['high'].values\n",
    "    lows = df['low'].values\n",
    "    L = len(df)\n",
    "    for i in range(L - n):\n",
    "        p0 = price[i]\n",
    "        up = p0 * (1 + up_mult[i])\n",
    "        dn = p0 * (1 - dn_mult[i])\n",
    "        hi_path = highs[i+1:i+n+1]\n",
    "        lo_path = lows[i+1:i+n+1]\n",
    "        hit_up = np.where(hi_path >= up)[0]\n",
    "        hit_dn = np.where(lo_path <= dn)[0]\n",
    "        if hit_up.size and (not hit_dn.size or hit_up[0] < hit_dn[0]):\n",
    "            y[i] = 1\n",
    "        elif hit_dn.size and (not hit_up.size or hit_dn[0] < hit_up[0]):\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 2\n",
    "    labels = pd.Series(y, index=df.index, name='label')\n",
    "    labels.iloc[-n:] = 2\n",
    "    return labels\n",
    "\n",
    "labels = triple_barrier_labels(base, n=N_HORIZON, k_up=K_UP, k_dn=K_DN, atr_len=ATR_LEN)\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fff6f9",
   "metadata": {},
   "source": [
    "## 6) Feature engineering (rich set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame):\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    # Price returns\n",
    "    X['ret_1'] = df['close'].pct_change()\n",
    "    for lag in [2,4,8,16,32,64,96]:\n",
    "        X[f'ret_{lag}'] = df['close'].pct_change(lag)\n",
    "    # Volatility\n",
    "    atr14 = compute_atr(df, atr_len=14)\n",
    "    X['atr14p'] = (atr14/df['close']).replace([np.inf,-np.inf], np.nan)\n",
    "    X['rv_24'] = np.log(df['close']).diff().rolling(96).std()\n",
    "    # EMAs\n",
    "    ema8 = df['close'].ewm(span=8).mean(); ema21 = df['close'].ewm(span=21).mean()\n",
    "    X['ema_diff'] = ema8 - ema21\n",
    "    # RSI\n",
    "    delta = df['close'].diff(); up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    rs = up.ewm(span=14).mean() / (down.ewm(span=14).mean() + 1e-9)\n",
    "    X['rsi14'] = 100 - 100/(1+rs)\n",
    "    # Spreads / premiums\n",
    "    if 'mark_close' in df.columns: X['mark_spread_p'] = (df['mark_close'] - df['close'])/df['close']\n",
    "    if 'index_close' in df.columns: X['index_spread_p'] = (df['index_close'] - df['close'])/df['close']\n",
    "    if 'premium_close' in df.columns: X['premium_chg'] = df['premium_close'].pct_change()\n",
    "    # Metrics (changes)\n",
    "    if 'sum_open_interest' in df.columns: X['oi_chg_p'] = df['sum_open_interest'].pct_change().fillna(0)\n",
    "    if 'sum_taker_long_short_vol_ratio' in df.columns: X['taker_ls_chg_p'] = df['sum_taker_long_short_vol_ratio'].pct_change().fillna(0)\n",
    "    # Order flow\n",
    "    if 'ofi' in df.columns: X['ofi'] = df['ofi']\n",
    "    if 'trades_count' in df.columns: X['trades_count'] = df['trades_count']\n",
    "    if 'vwap' in df.columns: X['vwap_close_spread'] = (df['vwap'] - df['close'])/df['close']\n",
    "    # Depth\n",
    "    if 'bd_notional_sum' in df.columns: X['bd_notional_sum'] = df['bd_notional_sum']\n",
    "    # Volume-scaled features\n",
    "    X['vol'] = df['volume'] if 'volume' in df.columns else 0\n",
    "    X['vol_med_24'] = X['vol'].rolling(96).median().replace(0,np.nan)\n",
    "    X['vol_surge'] = X['vol']/(X['vol_med_24']+1e-9)\n",
    "    # Clean\n",
    "    X = X.replace([np.inf,-np.inf], np.nan).fillna(method='bfill').fillna(0)\n",
    "    return X\n",
    "\n",
    "X = engineer_features(base)\n",
    "print('Feature matrix shape:', X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989fa63",
   "metadata": {},
   "source": [
    "## 7) Train LightGBM and calibrate (time-based split for calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b58455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss, log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Reserve the last ~15% for calibration (time-based holdout)\n",
    "n = len(X)\n",
    "cal_size = max(int(n * 0.15), 500)\n",
    "split_idx = n - cal_size\n",
    "\n",
    "X_tr, y_tr = X.iloc[:split_idx], labels.iloc[:split_idx]\n",
    "X_cal, y_cal = X.iloc[split_idx:], labels.iloc[split_idx:]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr.values)\n",
    "y_cal_enc = le.transform(y_cal.values)\n",
    "\n",
    "params = dict(objective='multiclass', num_class=3, learning_rate=0.03, n_estimators=1200,\n",
    "              num_leaves=96, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "clf = lgb.LGBMClassifier(**params)\n",
    "clf.fit(X_tr, y_tr_enc, eval_set=[(X_cal, y_cal_enc)], eval_metric='multi_logloss', verbose=False)\n",
    "\n",
    "# Isotonic calibration on tail holdout\n",
    "cal = CalibratedClassifierCV(clf, method='isotonic', cv='prefit')\n",
    "cal.fit(X_cal, y_cal_enc)\n",
    "\n",
    "probs_cal = cal.predict_proba(X_cal)\n",
    "up_idx = list(le.classes_).index(1) if 1 in le.classes_ else 0\n",
    "print('Calibration Brier (UP on holdout):', brier_score_loss((y_cal_enc==1).astype(int), probs_cal[:, up_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491e678",
   "metadata": {},
   "source": [
    "## 8) Save artifacts to Drive & predict() stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(ARTIFACT_DIR/'lgbm_calibrated.pkl', 'wb') as f:\n",
    "    pickle.dump(cal, f)\n",
    "with open(ARTIFACT_DIR/'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(ARTIFACT_DIR/'feature_columns.json', 'w') as f:\n",
    "    f.write(json.dumps(list(X.columns)))\n",
    "print('Saved artifacts to', ARTIFACT_DIR)\n",
    "\n",
    "def predict_live(latest_row: pd.Series, calibrated_model, label_encoder, feature_columns):\n",
    "    Xr = latest_row[feature_columns].values.reshape(1,-1)\n",
    "    proba = calibrated_model.predict_proba(Xr)[0]\n",
    "    classes = list(label_encoder.classes_)\n",
    "    out = {}\n",
    "    for cname, cid in [('P_down',0), ('P_up',1), ('P_neutral',2)]:\n",
    "        out[cname] = float(proba[classes.index(cid)]) if cid in classes else np.nan\n",
    "    return out\n",
    "\n",
    "res = predict_live(X.iloc[-1], cal, le, X.columns)\n",
    "print('Latest probabilities:', res)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
