{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bdd88b3",
   "metadata": {},
   "source": [
    "\n",
    "# Transformer — 15‑min Direction Using Close, Order Book Depth, and Metrics (OI RSI16)\n",
    "\n",
    "**Inputs (exact schemas):**\n",
    "- **klines**: `open_time, open, high, low, close, volume, close_time, quote_volume, count, taker_buy_volume, taker_buy_quote_volume, ignore`\n",
    "- **bookdepth**: `timestamp, percentage, depth, notional`\n",
    "- **metrics**: `create_time, symbol, sum_open_interest, sum_open_interest_value, count_toptrader_long_short_ratio, sum_toptrader_long_short_ratio, count_long_short_ratio, sum_taker_long_short_vol_ratio`\n",
    "\n",
    "**What we use:**\n",
    "- 15‑min **close** from `klines`\n",
    "- `bookdepth` aggregated to 15‑min: **bd_notional_sum**, **bd_depth_sum**, **bd_notional_max** (proxy for large resting orders)\n",
    "- `metrics` resampled to 15‑min + **OI RSI(16)** computed from `sum_open_interest`\n",
    "\n",
    "**Model:** Transformer encoder → binary P(up) over next *N* bars.\n",
    "\n",
    "**Memory:** `bookdepth` streamed in chunks (`pyarrow.ParquetFile.iter_batches`) and cached per file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3a59e",
   "metadata": {},
   "source": [
    "## 0) Pin dependencies (run ONCE), then Runtime → Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q uninstall -y torch torchvision torchaudio pyarrow\n",
    "%pip -q install pandas==2.2.2 pyarrow==16.1.0 fastparquet==2024.5.0\n",
    "%pip -q install --index-url https://download.pytorch.org/whl/cu126 torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n",
    "import os, pandas, pyarrow, torch\n",
    "print('OK:', pandas.__version__, pyarrow.__version__, torch.__version__)\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6370169",
   "metadata": {},
   "source": [
    "## 1) Setup & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, gc, math, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_ROOT = Path('/content/drive/MyDrive/binance_data')\n",
    "CACHE_DIR = Path('/content/drive/MyDrive/binance_cache')\n",
    "ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "assert DATA_ROOT.exists(), f'Missing data root: {DATA_ROOT}'\n",
    "BAR_FREQ = '15min'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d0172",
   "metadata": {},
   "source": [
    "## 2) Discover klines, bookDepth, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca98bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_manifest(root: Path) -> dict:\n",
    "    m = {k: [] for k in ['klines','bookdepth','metrics']}\n",
    "    for p in root.rglob('*.parquet'):\n",
    "        name = p.name.lower()\n",
    "        if 'bookdepth' in name or 'orderbook' in name:\n",
    "            m['bookdepth'].append(str(p))\n",
    "        elif 'metric' in name:\n",
    "            m['metrics'].append(str(p))\n",
    "        elif 'kline' in name:\n",
    "            m['klines'].append(str(p))\n",
    "    for k in m:\n",
    "        m[k] = sorted(m[k])\n",
    "    return m\n",
    "\n",
    "manifest = build_manifest(DATA_ROOT)\n",
    "for k,v in manifest.items():\n",
    "    print(f\"{k}: {len(v)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d9929",
   "metadata": {},
   "source": [
    "## 3) Helpers — robust time handling & streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "def _cache_path(kind: str, src_path: str) -> Path:\n",
    "    fn = Path(src_path).name.replace('.parquet', f'.{kind}.15m.parquet')\n",
    "    return CACHE_DIR / fn\n",
    "\n",
    "def to_utc_index(df, time_col: str, reconstruct_from_close=False):\n",
    "    if reconstruct_from_close and time_col == 'open_time':\n",
    "        if 'close_time' not in df.columns:\n",
    "            raise ValueError('close_time required to reconstruct open_time')\n",
    "        close_ts = pd.to_datetime(df['close_time'], errors='coerce', utc=True)\n",
    "        df = df.copy(); df['open_time'] = close_ts - pd.Timedelta('15min')\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce', utc=False)\n",
    "    df = df.dropna(subset=[time_col]).sort_values(time_col).drop_duplicates(time_col)\n",
    "    df = df.set_index(time_col)\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(f'Index is not DatetimeIndex after setting {time_col}')\n",
    "    df.index = df.index.tz_localize('UTC') if df.index.tz is None else df.index.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "def load_concat(files, columns):\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f, columns=columns, engine='pyarrow')\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print('Failed to load', f, e)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def stream_parquet_batches(path, columns, batch_size=200_000):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    for batch in pf.iter_batches(batch_size=batch_size, columns=columns):\n",
    "        yield batch.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800647b",
   "metadata": {},
   "source": [
    "## 4) Stream & cache bookDepth → 15m features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b893bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_bookdepth_file_streaming(path):\n",
    "    outs = []\n",
    "    cols = ['timestamp','percentage','depth','notional']\n",
    "    for chunk in stream_parquet_batches(path, columns=cols, batch_size=200_000):\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        if any(c not in chunk.columns for c in cols):\n",
    "            continue\n",
    "        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], utc=True, errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['timestamp']).set_index('timestamp')\n",
    "        for c in ('percentage','depth','notional'):\n",
    "            chunk[c] = pd.to_numeric(chunk[c], errors='coerce').astype('float32')\n",
    "        bins = chunk.index.floor('15min')\n",
    "        bd_notional_sum = chunk['notional'].groupby(bins).sum(min_count=1)\n",
    "        bd_depth_sum    = chunk['depth'].groupby(bins).sum(min_count=1)\n",
    "        bd_notional_max = chunk['notional'].groupby(bins).max()\n",
    "        out = pd.DataFrame({\n",
    "            'bd_notional_sum': bd_notional_sum.astype('float32'),\n",
    "            'bd_depth_sum':    bd_depth_sum.astype('float32'),\n",
    "            'bd_notional_max': bd_notional_max.astype('float32'),\n",
    "        })\n",
    "        out.index.name = 'time'\n",
    "        outs.append(out)\n",
    "    if not outs:\n",
    "        return pd.DataFrame()\n",
    "    df_all = pd.concat(outs)\n",
    "    return df_all.groupby(level=0, sort=True).sum()\n",
    "\n",
    "def process_bookdepth(files):\n",
    "    outs = []\n",
    "    for p in files:\n",
    "        cpath = _cache_path('bookdepth', p)\n",
    "        if cpath.exists():\n",
    "            out = pd.read_parquet(cpath, engine='pyarrow')\n",
    "        else:\n",
    "            out = aggregate_bookdepth_file_streaming(p)\n",
    "            if out is not None and not out.empty:\n",
    "                out.sort_index(inplace=True)\n",
    "                out.to_parquet(cpath)\n",
    "        if out is not None and not out.empty:\n",
    "            outs.append(out)\n",
    "    if not outs:\n",
    "        return pd.DataFrame()\n",
    "    df_all = pd.concat(outs)\n",
    "    return df_all.groupby(level=0, sort=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8809ed4",
   "metadata": {},
   "source": [
    "## 5) Load klines (close), metrics (15m + OI RSI16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd72cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_cols = ['open_time','close_time','close']\n",
    "kl_raw = load_concat(manifest['klines'], columns=kl_cols)\n",
    "print('klines raw:', kl_raw.shape)\n",
    "kl_df = to_utc_index(kl_raw, time_col='open_time', reconstruct_from_close=True)\n",
    "kl_df = kl_df[['close']].astype('float32')\n",
    "print('klines aligned:', kl_df.shape)\n",
    "\n",
    "mt_cols = ['create_time','symbol','sum_open_interest','sum_open_interest_value',\n",
    "           'count_toptrader_long_short_ratio','sum_toptrader_long_short_ratio',\n",
    "           'count_long_short_ratio','sum_taker_long_short_vol_ratio']\n",
    "mt_raw = load_concat(manifest['metrics'], columns=mt_cols)\n",
    "mt_df  = to_utc_index(mt_raw, time_col='create_time') if not mt_raw.empty else pd.DataFrame()\n",
    "if not mt_df.empty:\n",
    "    mt_15 = mt_df[['sum_open_interest','sum_open_interest_value',\n",
    "                   'count_toptrader_long_short_ratio','sum_toptrader_long_short_ratio',\n",
    "                   'count_long_short_ratio','sum_taker_long_short_vol_ratio']].astype('float32').resample('15min').ffill()\n",
    "else:\n",
    "    mt_15 = pd.DataFrame()\n",
    "print('metrics 15m:', mt_15.shape)\n",
    "\n",
    "def rsi_series(x: pd.Series, length=16):\n",
    "    delta = x.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1/length, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/length, adjust=False).mean()\n",
    "    rs = roll_up / (roll_down + 1e-9)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "if not mt_15.empty and 'sum_open_interest' in mt_15.columns:\n",
    "    mt_15['oi_rsi16'] = rsi_series(mt_15['sum_open_interest'].astype('float32'), length=16).astype('float32')\n",
    "else:\n",
    "    if not mt_15.empty:\n",
    "        mt_15['oi_rsi16'] = np.nan\n",
    "    else:\n",
    "        mt_15 = pd.DataFrame({'oi_rsi16': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb86b2",
   "metadata": {},
   "source": [
    "## 6) Build bookDepth 15m aggregates (streamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180afe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_15 = process_bookdepth(manifest['bookdepth'])\n",
    "print('bookdepth15:', bd_15.shape)\n",
    "bd_15.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2617a",
   "metadata": {},
   "source": [
    "## 7) Merge unified 15m frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8544248",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = kl_df.copy()\n",
    "def safe_join(left, right):\n",
    "    return left.join(right, how='left') if right is not None and not right.empty else left\n",
    "\n",
    "base = safe_join(base, mt_15)\n",
    "base = safe_join(base, bd_15)\n",
    "base = base.sort_index().replace([np.inf,-np.inf], np.nan).fillna(method='ffill').dropna()\n",
    "print('Unified base shape:', base.shape)\n",
    "base.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb096fe",
   "metadata": {},
   "source": [
    "## 8) Features & labels (binary up/down in next N bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HORIZON = 16\n",
    "NEUTRAL_EPS = 0.0\n",
    "\n",
    "feat_cols = ['close','sum_open_interest','sum_open_interest_value',\n",
    "             'count_toptrader_long_short_ratio','sum_toptrader_long_short_ratio',\n",
    "             'count_long_short_ratio','sum_taker_long_short_vol_ratio',\n",
    "             'oi_rsi16','bd_notional_sum','bd_depth_sum','bd_notional_max']\n",
    "\n",
    "X = base[feat_cols].copy()\n",
    "X['close_ret_1'] = X['close'].pct_change().fillna(0).astype('float32')\n",
    "future_close = base['close'].shift(-N_HORIZON)\n",
    "fwd_ret = (future_close - base['close']) / base['close']\n",
    "y = (fwd_ret > NEUTRAL_EPS).astype('int64')\n",
    "df = X.copy(); df['target'] = y\n",
    "df = df.dropna()\n",
    "print('Final dataset:', df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53c5e6",
   "metadata": {},
   "source": [
    "## 9) Time split & sequence windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d759ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "LOOKBACK = 128\n",
    "STRIDE = 1\n",
    "n = len(df)\n",
    "i_tr = int(n*0.7); i_va = int(n*0.85)\n",
    "train_df = df.iloc[:i_tr]; val_df = df.iloc[i_tr:i_va]; test_df = df.iloc[i_va:]\n",
    "features = [c for c in df.columns if c != 'target']\n",
    "scaler = StandardScaler().fit(train_df[features])\n",
    "\n",
    "def make_sequences(frame):\n",
    "    Xn = scaler.transform(frame[features]).astype('float32')\n",
    "    y  = frame['target'].astype('int64').values\n",
    "    X_seq, y_seq = [], []\n",
    "    for t in range(LOOKBACK, len(frame), STRIDE):\n",
    "        X_seq.append(Xn[t-LOOKBACK:t, :])\n",
    "        y_seq.append(y[t])\n",
    "    return np.stack(X_seq), np.array(y_seq)\n",
    "\n",
    "Xtr, Ytr = make_sequences(train_df)\n",
    "Xva, Yva = make_sequences(val_df)\n",
    "Xte, Yte = make_sequences(test_df)\n",
    "print('Seq shapes:', Xtr.shape, Xva.shape, Xte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607e074",
   "metadata": {},
   "source": [
    "## 10) Transformer — train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, math\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        L = x.size(1); return x + self.pe[:, :L, :]\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        self.posenc = PositionalEncoding(d_model)\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))\n",
    "    def forward(self, x):\n",
    "        z = self.input_proj(x); z = self.posenc(z); z = self.encoder(z)\n",
    "        return self.head(z[:, -1, :]).squeeze(-1)\n",
    "\n",
    "num_features = len(features)\n",
    "model = TimeSeriesTransformer(num_features=num_features).to(device)\n",
    "train_ds = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(Ytr))\n",
    "val_ds   = TensorDataset(torch.from_numpy(Xva), torch.from_numpy(Yva))\n",
    "test_ds  = TensorDataset(torch.from_numpy(Xte), torch.from_numpy(Yte))\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False)\n",
    "criterion = nn.BCEWithLogitsLoss(); optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval(); loss_sum=0; n=0; correct=0\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            xb = xb.to(device); yb = yb.float().to(device)\n",
    "            logits = model(xb); loss = criterion(logits, yb)\n",
    "            loss_sum += loss.item()*len(xb)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            correct += (preds.cpu()==yb.long().cpu()).sum().item(); n += len(xb)\n",
    "    return loss_sum/n, correct/n\n",
    "\n",
    "from pathlib import Path\n",
    "ARTIFACT_DIR = Path('/content/drive/MyDrive/binance_models')\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "BEST_PATH = ARTIFACT_DIR/'transformer_close_depth_metrics_clean.pth'\n",
    "best_val = float('inf')\n",
    "for ep in range(1, 13):\n",
    "    model.train()\n",
    "    for xb,yb in train_loader:\n",
    "        xb = xb.to(device); yb = yb.float().to(device)\n",
    "        optimizer.zero_grad(); logits = model(xb); loss = criterion(logits, yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "    vloss, vacc = evaluate(val_loader)\n",
    "    print(f'Epoch {ep:02d}  val_loss={vloss:.4f}  val_acc={vacc:.3f}')\n",
    "    if vloss < best_val:\n",
    "        best_val = vloss\n",
    "        torch.save({'model_state': model.state_dict(), 'features': features, 'config': {'lookback': 128, 'horizon': 16}}, BEST_PATH)\n",
    "        print('  ✓ saved best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c4c2a",
   "metadata": {},
   "source": [
    "## 11) Evaluate on test & save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tloss, tacc = evaluate(DataLoader(test_ds, batch_size=512, shuffle=False))\n",
    "print('Test  loss:', round(tloss,4), ' acc:', round(tacc,3))\n",
    "\n",
    "import joblib, json\n",
    "joblib.dump(scaler, '/content/drive/MyDrive/binance_models/scaler_close_depth_metrics_clean.gz')\n",
    "with open('/content/drive/MyDrive/binance_models/features_close_depth_metrics_clean.json','w') as f:\n",
    "    json.dump(features, f)\n",
    "print('Artifacts saved to /content/drive/MyDrive/binance_models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f838c7",
   "metadata": {},
   "source": [
    "## 12) Inference helper — latest window → P(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from pathlib import Path\n",
    "def predict_latest_prob_up(model_path='/content/drive/MyDrive/binance_models/transformer_close_depth_metrics_clean.pth'):\n",
    "    ckpt = torch.load(model_path, map_location='cpu')\n",
    "    feats = ckpt['features']; lookback = ckpt['config']['lookback']\n",
    "    window = df[feats].iloc[-lookback:]\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import joblib\n",
    "    scaler = joblib.load('/content/drive/MyDrive/binance_models/scaler_close_depth_metrics_clean.gz')\n",
    "    Xw = scaler.transform(window).astype('float32')\n",
    "    xb = torch.from_numpy(Xw).unsqueeze(0)\n",
    "    class TimeSeriesTransformer(torch.nn.Module):\n",
    "        def __init__(self, num_features, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "            super().__init__(); import math, torch.nn as nn\n",
    "            class PositionalEncoding(nn.Module):\n",
    "                def __init__(self, d_model, max_len=10000):\n",
    "                    super().__init__(); import torch\n",
    "                    pe = torch.zeros(max_len, d_model)\n",
    "                    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "                    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "                    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "                    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "                    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "                def forward(self, x):\n",
    "                    L = x.size(1); return x + self.pe[:, :L, :]\n",
    "            self.input_proj = nn.Linear(num_features, d_model)\n",
    "            self.posenc = PositionalEncoding(d_model)\n",
    "            enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "            self.encoder = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "            self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))\n",
    "        def forward(self, x):\n",
    "            z = self.input_proj(x); z = self.posenc(z); z = self.encoder(z)\n",
    "            return self.head(z[:, -1, :]).squeeze(-1)\n",
    "    m = TimeSeriesTransformer(num_features=len(feats))\n",
    "    m.load_state_dict(ckpt['model_state']); m.eval()\n",
    "    with torch.no_grad():\n",
    "        p_up = torch.sigmoid(m(xb)).item()\n",
    "    return p_up\n",
    "\n",
    "print('P(up) next N bars:', round(predict_latest_prob_up(), 4))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
