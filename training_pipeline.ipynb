{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9bf331",
   "metadata": {},
   "source": [
    "# BTC Turning-Point Prediction Pipeline\n",
    "\n",
    "**Notebook purpose:** a complete, runnable skeleton to train probabilistic turning-point models\n",
    "using historical Binance Futures-derived 15m candles and related endpoints. The notebook contains:\n",
    "\n",
    "- Data ingestion placeholders (replace with paths to your CSVs)\n",
    "- Volatility-adaptive triple-barrier labeling\n",
    "- Feature engineering examples\n",
    "- LightGBM baseline training + calibration\n",
    "- PyTorch Transformer encoder model skeleton and training loop\n",
    "- Saving and inference stubs\n",
    "\n",
    "**Notes:** This notebook is a starting point. Replace file paths and expand feature engineering as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, pickle, math, time\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, brier_score_loss\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print('pandas', pd.__version__, 'numpy', np.__version__, 'torch', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda61bb0",
   "metadata": {},
   "source": [
    "## 1) Data loading & preprocessing\n",
    "\n",
    "Replace the placeholder paths with your CSVs exported from Binance. This example demonstrates how to load `klines.csv` (15m) and basic trades/aggtrades merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: load klines (15m). Replace path with your file.\n",
    "KLINES_CSV = '/mnt/data/klines_15m_sample.csv'  # replace with your real file\n",
    "if os.path.exists(KLINES_CSV):\n",
    "    klines = pd.read_csv(KLINES_CSV, parse_dates=['open_time'])\n",
    "    klines.set_index('open_time', inplace=True)\n",
    "    klines = klines.sort_index()\n",
    "    print('Loaded klines', klines.shape)\n",
    "else:\n",
    "    dates = pd.date_range('2025-01-01', periods=2000, freq='15T', tz='UTC')\n",
    "    price = 50000 + np.cumsum(np.random.randn(len(dates))*50)\n",
    "    klines = pd.DataFrame(index=dates)\n",
    "    klines['open'] = price + np.random.randn(len(dates))*5\n",
    "    klines['high'] = klines['open'] + np.abs(np.random.randn(len(dates))*20)\n",
    "    klines['low']  = klines['open'] - np.abs(np.random.randn(len(dates))*20)\n",
    "    klines['close'] = price + np.random.randn(len(dates))*5\n",
    "    klines['volume'] = np.abs(np.random.randn(len(dates))*10)\n",
    "    print('Created synthetic klines', klines.shape)\n",
    "\n",
    "if klines.index.tz is None:\n",
    "    klines = klines.tz_localize('UTC')\n",
    "else:\n",
    "    klines = klines.tz_convert('UTC')\n",
    "klines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aba605",
   "metadata": {},
   "source": [
    "## 2) Triple-barrier labeling (volatility-adaptive)\n",
    "\n",
    "This labels each bar as UP/DOWN/NEUTRAL based on ATR-scaled upper/lower barriers and a time horizon `n` bars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633630cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atr(df, atr_len=14):\n",
    "    high = df['high']; low = df['low']; close = df['close']\n",
    "    tr1 = (high - low).abs()\n",
    "    tr2 = (high - close.shift()).abs()\n",
    "    tr3 = (low - close.shift()).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(span=atr_len, adjust=False).mean()\n",
    "    return atr\n",
    "\n",
    "def triple_barrier_labels(df, n=16, k_up=1.5, k_dn=1.5, atr_len=14, drop_na=True):\n",
    "    # returns labels: 1=UP, 0=DOWN, 2=NEUTRAL\n",
    "    df2 = df.copy()\n",
    "    atr = compute_atr(df2, atr_len=atr_len)\n",
    "    df2['atr'] = atr\n",
    "    price = df2['close'].values\n",
    "    up_mult = (k_up * df2['atr'] / df2['close']).fillna(method='bfill').values\n",
    "    dn_mult = (k_dn * df2['atr'] / df2['close']).fillna(method='bfill').values\n",
    "    y = np.full(len(df2), 2, dtype=np.int8)\n",
    "    highs = df2['high'].values\n",
    "    lows = df2['low'].values\n",
    "    L = len(df2)\n",
    "    for i in range(L - n):\n",
    "        p0 = price[i]\n",
    "        up = p0 * (1 + up_mult[i])\n",
    "        dn = p0 * (1 - dn_mult[i])\n",
    "        hi_path = highs[i+1:i+n+1]\n",
    "        lo_path = lows[i+1:i+n+1]\n",
    "        hit_up = np.where(hi_path >= up)[0]\n",
    "        hit_dn = np.where(lo_path <= dn)[0]\n",
    "        if hit_up.size and (not hit_dn.size or hit_up[0] < hit_dn[0]):\n",
    "            y[i] = 1\n",
    "        elif hit_dn.size and (not hit_up.size or hit_dn[0] < hit_up[0]):\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 2\n",
    "    labels = pd.Series(y, index=df2.index, name='label')\n",
    "    if drop_na:\n",
    "        labels.iloc[-n:] = 2\n",
    "    return labels\n",
    "\n",
    "labels = triple_barrier_labels(klines, n=16, k_up=1.5, k_dn=1.5, atr_len=14)\n",
    "print(labels.value_counts())\n",
    "labels.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20e84c",
   "metadata": {},
   "source": [
    "## 3) Feature engineering (examples)\n",
    "\n",
    "Create a set of rolling and technical features. Expand these with trade/aggtrades/bookdepth features from your CSVs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X['close'] = df['close']\n",
    "    X['ret_1'] = df['close'].pct_change()\n",
    "    for lag in [1,2,4,8,16,32]:\n",
    "        X[f'ret_{lag}'] = df['close'].pct_change(lag)\n",
    "    X['atr14'] = compute_atr(df, atr_len=14)\n",
    "    X['rv_24'] = np.log(df['close']).diff().rolling(window=96).std()\n",
    "    X['ema_8'] = df['close'].ewm(span=8).mean()\n",
    "    X['ema_21'] = df['close'].ewm(span=21).mean()\n",
    "    X['ema_diff'] = X['ema_8'] - X['ema_21']\n",
    "    delta = df['close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    rs = up.ewm(span=14).mean() / (down.ewm(span=14).mean() + 1e-12)\n",
    "    X['rsi14'] = 100 - 100/(1+rs)\n",
    "    X['mom_12'] = df['close'] / df['close'].shift(12) - 1\n",
    "    X['vol'] = df['volume']\n",
    "    X['vol_24'] = X['vol'].rolling(window=96).median().fillna(method='bfill')\n",
    "    X['vol_surge'] = X['vol'] / (X['vol_24'] + 1e-9)\n",
    "    X = X.fillna(method='bfill').fillna(0)\n",
    "    return X\n",
    "\n",
    "X = make_features(klines)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65f554",
   "metadata": {},
   "source": [
    "## 4) Train/test split\n",
    "\n",
    "Using your specified date ranges: Train 2025-02-06 → 2025-06-06, Backtest 2025-06-07 → 2025-08-25. Adjust if your data spans different years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02442960",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = pd.Timestamp('2025-02-06', tz='UTC')\n",
    "train_end   = pd.Timestamp('2025-06-06', tz='UTC')\n",
    "test_start  = pd.Timestamp('2025-06-07', tz='UTC')\n",
    "test_end    = pd.Timestamp('2025-08-25', tz='UTC')\n",
    "\n",
    "train_mask = (X.index >= train_start) & (X.index <= train_end)\n",
    "test_mask  = (X.index >= test_start) & (X.index <= test_end)\n",
    "\n",
    "if train_mask.sum() < 50:\n",
    "    print('Warning: train mask empty or too small for these example/synthetic data. Using first 70% for train.')\n",
    "    split = int(len(X)*0.7)\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = labels.iloc[:split], labels.iloc[split:]\n",
    "else:\n",
    "    X_train, X_test = X.loc[train_mask], X.loc[test_mask]\n",
    "    y_train, y_test = labels.loc[train_mask], labels.loc[test_mask]\n",
    "\n",
    "print('Train rows:', X_train.shape[0], 'Test rows:', X_test.shape[0])\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517092c",
   "metadata": {},
   "source": [
    "## 5) LightGBM baseline (multiclass + calibration)\n",
    "\n",
    "Train a LightGBM multiclass classifier and calibrate with isotonic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train.values)\n",
    "y_test_enc  = le.transform(y_test.values)\n",
    "\n",
    "keep_neutral = True\n",
    "\n",
    "if not keep_neutral:\n",
    "    mask_tr = y_train_enc != 2\n",
    "    mask_te = y_test_enc != 2\n",
    "    X_train_b = X_train.loc[mask_tr]; y_train_b = y_train_enc[mask_tr]\n",
    "    X_test_b  = X_test.loc[mask_te];  y_test_b  = y_test_enc[mask_te]\n",
    "else:\n",
    "    X_train_b, y_train_b = X_train, y_train_enc\n",
    "    X_test_b,  y_test_b  = X_test,  y_test_enc\n",
    "\n",
    "print('Classes:', np.unique(y_train_b))\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass' if keep_neutral else 'multiclass',\n",
    "    'num_class': 3 if keep_neutral else 2,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 1000,\n",
    "    'num_leaves': 64,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1.0,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "dtrain = lgb.Dataset(X_train_b, label=y_train_b)\n",
    "dval = lgb.Dataset(X_test_b, label=y_test_b, reference=dtrain)\n",
    "bst = lgb.train(params, dtrain, valid_sets=[dtrain,dval], early_stopping_rounds=50, verbose_eval=50)\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "clf = lgb.LGBMClassifier(**{k:v for k,v in params.items() if k in ['learning_rate','n_estimators','num_leaves','subsample','colsample_bytree','reg_alpha','reg_lambda','random_state','n_jobs']})\n",
    "clf.fit(X_train_b, y_train_b)\n",
    "calibrator = CalibratedClassifierCV(clf, method='isotonic', cv=3)\n",
    "calibrator.fit(X_train_b, y_train_b)\n",
    "\n",
    "probs = calibrator.predict_proba(X_test_b)\n",
    "if probs.shape[1] == 3:\n",
    "    proba_up = probs[:,1]; proba_down = probs[:,0]; proba_neu = probs[:,2]\n",
    "else:\n",
    "    proba_up = probs[:,1]; proba_down = probs[:,0]; proba_neu = np.zeros_like(proba_up)\n",
    "\n",
    "print('Test Brier for UP class:', brier_score_loss((y_test_b==1).astype(int), proba_up))\n",
    "print('Test ROC AUC (one-vs-rest) for UP:', roc_auc_score((y_test_b==1).astype(int), proba_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf1f66",
   "metadata": {},
   "source": [
    "## 6) Transformer model skeleton (PyTorch)\n",
    "\n",
    "This builds a small Transformer encoder to ingest sequence windows and output class probabilities. You can train it on many years by batching windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_df, y_ser, window=128, features=None):\n",
    "        self.X = X_df\n",
    "        self.y = y_ser\n",
    "        self.idx = np.arange(len(self.X))\n",
    "        self.window = window\n",
    "        self.features = X_df.columns.tolist() if features is None else features\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.X) - self.window)\n",
    "    def __getitem__(self, i):\n",
    "        start = i\n",
    "        end = i + self.window\n",
    "        x = self.X.iloc[start:end][self.features].values.astype('float32')\n",
    "        y = int(self.y.iloc[start + self.window - 1])\n",
    "        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class TimeTransformer(nn.Module):\n",
    "    def __init__(self, n_features, d_model=64, n_heads=4, num_layers=2, num_classes=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward= d_model*4, dropout=dropout, activation='gelu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1,2,0)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "features = X.columns.tolist()\n",
    "window = 64\n",
    "ds = SeqDataset(X.fillna(0), labels.fillna(2).astype(int), window=window, features=features)\n",
    "if len(ds) > 0:\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "    model = TimeTransformer(n_features=len(features), d_model=64, n_heads=4, num_layers=2, num_classes=3)\n",
    "    xb, yb = next(iter(loader))\n",
    "    logits = model(xb)\n",
    "    print('model output shape', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_ds, val_ds, epochs=5, lr=1e-4, device='cpu'):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "    best_loss = 1e9\n",
    "    for ep in range(epochs):\n",
    "        model.train(); tot=0; acc=0; loss_sum=0.0\n",
    "        for xb,yb in train_loader:\n",
    "            xb = xb.to(device); yb=yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward(); opt.step()\n",
    "            loss_sum += loss.item(); tot+=1\n",
    "        model.eval(); vloss=0; cnt=0\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb=xb.to(device); yb=yb.to(device)\n",
    "                logits = model(xb)\n",
    "                vloss += crit(logits, yb).item(); cnt+=1\n",
    "        vloss = vloss / max(1,cnt)\n",
    "        print(f'EP {ep+1}/{epochs} train_loss={loss_sum/tot:.4f} val_loss={vloss:.4f}')\n",
    "        if vloss < best_loss:\n",
    "            best_loss = vloss\n",
    "            torch.save(model.state_dict(), '/mnt/data/transformer_best.pth')\n",
    "    return model\n",
    "\n",
    "n_total = len(X)\n",
    "split_idx = int(n_total*0.7)\n",
    "train_ds = SeqDataset(X.iloc[:split_idx].fillna(0), labels.iloc[:split_idx].fillna(2).astype(int), window=window, features=features)\n",
    "val_ds = SeqDataset(X.iloc[split_idx:].fillna(0), labels.iloc[:split_idx].fillna(2).astype(int), window=window, features=features)\n",
    "if len(train_ds)>0 and len(val_ds)>0:\n",
    "    model = TimeTransformer(n_features=len(features), d_model=64, n_heads=4, num_layers=2, num_classes=3)\n",
    "    trained = train_transformer(model, train_ds, val_ds, epochs=2, lr=3e-4, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d6ec2",
   "metadata": {},
   "source": [
    "## 7) Save models & inference stub\n",
    "\n",
    "Save LightGBM and Transformer artifacts; provide a `predict()` function that given latest features and sequence returns calibrated probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d147ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/data/lgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "with open('/mnt/data/calibrator.pkl', 'wb') as f:\n",
    "    pickle.dump(calibrator, f)\n",
    "print('Saved LightGBM and calibrator.')\n",
    "\n",
    "\n",
    "def predict_live(latest_df_row, recent_seq_df, feature_list, device='cpu'):\n",
    "    with open('/mnt/data/lgbm_model.pkl','rb') as f:\n",
    "        lgbm = pickle.load(f)\n",
    "    with open('/mnt/data/calibrator.pkl','rb') as f:\n",
    "        cal = pickle.load(f)\n",
    "    Xr = latest_df_row[feature_list].values.reshape(1,-1)\n",
    "    proba_lgb = cal.predict_proba(Xr)[0]\n",
    "    model = TimeTransformer(n_features=len(feature_list), d_model=64, n_heads=4, num_layers=2, num_classes=3)\n",
    "    model.load_state_dict(torch.load('/mnt/data/transformer_best.pth', map_location=device))\n",
    "    model.eval()\n",
    "    seq = torch.from_numpy(recent_seq_df[feature_list].values.astype('float32')).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(seq)\n",
    "        proba_tr = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    eps = 1e-9\n",
    "    logit_lgb = np.log(proba_lgb + eps)\n",
    "    logit_tr  = np.log(proba_tr  + eps)\n",
    "    logit_avg = (logit_lgb + logit_tr) / 2.0\n",
    "    proba_ens = np.exp(logit_avg) / np.exp(logit_avg).sum()\n",
    "    return {'proba_up': float(proba_ens[1]), 'proba_down': float(proba_ens[0]), 'proba_neutral': float(proba_ens[2])}\n",
    "\n",
    "if len(X_test)>0:\n",
    "    row = X_test.iloc[-1]\n",
    "    seq_df = X_test.iloc[-128:] if len(X_test)>=128 else X_test\n",
    "    print('Example live prediction:', predict_live(row, seq_df, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e70803f",
   "metadata": {},
   "source": [
    "Notebook saved to `/mnt/data/training_pipeline.ipynb`. You can download it from the link below."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
